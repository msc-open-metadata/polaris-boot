{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1041ae6f",
   "metadata": {},
   "source": [
    "![iceberg-logo](https://www.apache.org/logos/res/iceberg/iceberg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247fb2ab",
   "metadata": {},
   "source": [
    "### [Docker, Spark, and Iceberg: The Fastest Way to Try Iceberg!](https://tabular.io/blog/docker-spark-and-iceberg/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21411ff4-eeee-476d-ac2e-b2b727a1e2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark-opendic==0.3.18\n",
      "  Downloading pyspark_opendic-0.3.18-py3-none-any.whl (9.2 kB)\n",
      "Requirement already satisfied: requests>=2.32.3 in /usr/local/lib/python3.10/site-packages (from pyspark-opendic==0.3.18) (2.32.3)\n",
      "Requirement already satisfied: pandas>=2.2.3 in /usr/local/lib/python3.10/site-packages (from pyspark-opendic==0.3.18) (2.2.3)\n",
      "Requirement already satisfied: pydantic>=2.10.6 in /usr/local/lib/python3.10/site-packages (from pyspark-opendic==0.3.18) (2.10.6)\n",
      "Requirement already satisfied: pyspark>=3.5.5 in /opt/spark/python (from pyspark-opendic==0.3.18) (3.5.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas>=2.2.3->pyspark-opendic==0.3.18) (2025.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/site-packages (from pandas>=2.2.3->pyspark-opendic==0.3.18) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas>=2.2.3->pyspark-opendic==0.3.18) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/site-packages (from pandas>=2.2.3->pyspark-opendic==0.3.18) (2025.1)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/site-packages (from pydantic>=2.10.6->pyspark-opendic==0.3.18) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/site-packages (from pydantic>=2.10.6->pyspark-opendic==0.3.18) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/site-packages (from pydantic>=2.10.6->pyspark-opendic==0.3.18) (0.7.0)\n",
      "Collecting py4j==0.10.9.7\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.3->pyspark-opendic==0.3.18) (2025.1.31)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.3->pyspark-opendic==0.3.18) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.3->pyspark-opendic==0.3.18) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.3->pyspark-opendic==0.3.18) (2.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=2.2.3->pyspark-opendic==0.3.18) (1.17.0)\n",
      "Installing collected packages: py4j, pyspark-opendic\n",
      "Successfully installed py4j-0.10.9.7 pyspark-opendic-0.3.18\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark-opendic==0.3.18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8961c31e-19e6-4470-bbae-4611bf76f364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Running\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/08 17:56:47 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://7d3cdfff5e22:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff83d3d8d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def read_secret(secret_name):\n",
    "    \"\"\" Get `secret_name` from docker-compose secret store\"\"\"\n",
    "    secret_path = f\"/run/secrets/{secret_name}\"\n",
    "    try:\n",
    "        with open(secret_path, \"r\") as f:\n",
    "            return f.read().strip()  # Remove any trailing newline\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Secret {secret_name} not found.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "## DEFINE SENSITIVE VARIABLES\n",
    "POLARIS_CATALOG_NAME = 'AZURE_CATALOG'\n",
    "ENGINEER_CLIENT_ID = read_secret(\"engineer_client_id\")\n",
    "ENGINEER_CLIENT_SECRET =  read_secret(\"engineer_client_secret\")\n",
    "ADLS_IO=\"org.apache.iceberg.azure.adlsv2.ADLSFileIO\"\n",
    "FILE_IO=\"org.apache.iceberg.io.ResolvingFileIO\"\n",
    "\n",
    "def create_session(client_id, client_secret, scope, fileio_impl):\n",
    "    spark = (SparkSession.builder\n",
    "        .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.0,software.amazon.awssdk:bundle:2.28.17,software.amazon.awssdk:url-connection-client:2.28.17\")\n",
    "        .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\")\n",
    "        .config(\"spark.sql.catalog.polaris\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "        .config(\"spark.sql.catalog.polaris.type\", \"rest\")\n",
    "        .config(\"spark.sql.catalog.polaris.warehouse\", POLARIS_CATALOG_NAME)\n",
    "        .config(\"spark.sql.catalog.polaris.uri\", 'https://opendict.duckdns.org/api/catalog')\n",
    "        .config(\"spark.sql.catalog.polaris.credential\", f\"{client_id}:{client_secret}\")\n",
    "        .config(\"spark.sql.catalog.polaris.scope\", 'PRINCIPAL_ROLE:ALL')\n",
    "        .config(\"spark.sql.defaultCatalog\", \"polaris\")\n",
    "        .config(\"spark.sql.catalogImplementation\", \"in-memory\")\n",
    "        .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\")\n",
    "        .config(\"spark.sql.catalog.polaris.token-refresh-enabled\", \"true\")\n",
    "        .config(\"spark.sql.catalog.polaris.header.X-Iceberg-Access-Delegation\", 'vended-credentials')\n",
    "        .config(\"spark.sql.catalog.polaris.io-impl\", fileio_impl)\n",
    "        .config(\"spark.history.fs.logDirectory\", \"/home/iceberg/spark-events\")).getOrCreate()\n",
    "        \n",
    "    print(\"Spark Running\")\n",
    "    return spark\n",
    "\n",
    "\n",
    "## Start Spark Session\n",
    "spark = create_session(client_id=ENGINEER_CLIENT_ID, client_secret=ENGINEER_CLIENT_SECRET, scope='PRINCIPAL_ROLE:ALL',fileio_impl=ADLS_IO )\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192f7adc-e72e-4ae7-b436-4a3f1c7b8e57",
   "metadata": {},
   "source": [
    "## Setting up pyspark-opendic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27ccb2cb-8448-4111-b2af-bd3b19b92835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catalog initialized\n"
     ]
    }
   ],
   "source": [
    "from pyspark_opendic.catalog import OpenDicCatalog\n",
    "\n",
    "# Init polarisx catalog\n",
    "POLARIS_URI= \"https://opendict.duckdns.org/api\"\n",
    "catalog = OpenDicCatalog(spark, POLARIS_URI)\n",
    "print(\"Catalog initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30d98c61-b213-4bcd-b1ee-216dd8be9d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|      nyc|\n",
      "|   SYSTEM|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "catalog.sql(\"Show namespaces\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a68bd999-9f33-4c28-8568-dec8b3f6dbdb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_ROUTINE] Cannot resolve function `current_namespace` on search path [`system`.`builtin`, `system`.`session`, `polaris`].; line 1 pos 7",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msql\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSELECT current_namespace()\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2543\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2541\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2542\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2543\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2545\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2547\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2548\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/.ipython/profile_default/startup/00-prettytables.py:81\u001b[0m, in \u001b[0;36msql\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _to_table(df, num_rows\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mlimit)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _to_table(\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_ROUTINE] Cannot resolve function `current_namespace` on search path [`system`.`builtin`, `system`.`session`, `polaris`].; line 1 pos 7"
     ]
    }
   ],
   "source": [
    "%%sql\n",
    "SELECT current_namespace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354fa57d-4e4b-40e3-b560-afdf7abaadc5",
   "metadata": {},
   "source": [
    "### Define the schema for a andfunc object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99808c3e-7fef-42f6-9a1e-4e44b798c69a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>udoType</th>\n",
       "      <th>properties</th>\n",
       "      <th>createdTimestamp</th>\n",
       "      <th>lastUpdatedTimestamp</th>\n",
       "      <th>version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>function_v2</td>\n",
       "      <td>{'return_type': 'STRING', 'created_time': 'STRING', 'entity_version': 'STRING', 'uname': 'STRING', 'def': 'STRING', 'signature': 'STRING', 'runtime': 'STRING', 'language': 'STRING', 'packages': 'STRING', 'args': 'STRING', 'last_updated_time': 'STRING', 'comment': 'STRING', 'client_version': 'STRING'}</td>\n",
       "      <td>1970-01-01T00:00Z</td>\n",
       "      <td>1970-01-01T00:00Z</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       udoType                                                                                                                                                                                                                                                                                                     properties   createdTimestamp lastUpdatedTimestamp version\n",
       "0  function_v2  {'return_type': 'STRING', 'created_time': 'STRING', 'entity_version': 'STRING', 'uname': 'STRING', 'def': 'STRING', 'signature': 'STRING', 'runtime': 'STRING', 'language': 'STRING', 'packages': 'STRING', 'args': 'STRING', 'last_updated_time': 'STRING', 'comment': 'STRING', 'client_version': 'STRING'}  1970-01-01T00:00Z    1970-01-01T00:00Z    None"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalog.sql(\n",
    "    \"\"\"\n",
    "    DEFINE OPEN function_v2\n",
    "    props {\n",
    "        \"args\": \"MAP\",\n",
    "        \"language\": \"STRING\",\n",
    "        \"def\": \"string\",\n",
    "        \"comment\": \"string\",\n",
    "        \"packages\": \"list\",\n",
    "        \"runtime\": \"string\",\n",
    "        \"client_version\": \"int\",\n",
    "        \"signature\": \"STRING\",\n",
    "        \"return_type\": \"STRING\"\n",
    "    }\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a117eb1-0929-4e30-9dea-c6980411f991",
   "metadata": {},
   "source": [
    "### Create a new andfunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59b83c05-2e49-449c-9fc3-f644acf682c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "    \"error\": \"HTTP Error\",\n",
       "    \"details\": \"409 Client Error: Conflict for url: https://opendict.duckdns.org/api/opendic/v1/objects/function_v2\",\n",
       "    \"Catalog Response\": null\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "{\n",
       "    \"error\": \"HTTP Error\",\n",
       "    \"details\": \"409 Client Error: Conflict for url: https://opendict.duckdns.org/api/opendic/v1/objects/function_v2\",\n",
       "    \"Catalog Response\": null\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalog.sql(\n",
    " \"\"\"\n",
    " CREATE OPEN function_v2 foo\n",
    "    props {\n",
    "            \"args\": {\n",
    "                \"arg1\": \"int\", \n",
    "                \"arg2\": \"int\"\n",
    "                },\n",
    "            \"language\": \"python\",\n",
    "            \"def\": \"def foo(arg1, arg2):\\\\n\\\\n        return arg1 + arg2\",\n",
    "            \"packages\" : [\"numpy\", \"pandas\"],\n",
    "            \"comment\": \"test fun\",\n",
    "            \"runtime\": \"3.12\",\n",
    "            \"client_version\": 1,\n",
    "            \"return_type\": \"int\",\n",
    "            \"signature\": \"foo(arg1 str, arg2 int)\"\n",
    "        }\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc4ce21-1ded-4e92-8b3a-18f3d2264aa7",
   "metadata": {},
   "source": [
    "### Create Mapping to snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3023dc59-10d0-42fe-8fa8-c9662c9b2078",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog.sql(\n",
    "    \"\"\"\n",
    "    ADD OPEN MAPPING function_v2 PLATFORM snowflake\n",
    "    SYNTAX {\n",
    "        \"CREATE OR ALTER <type> <name>(<args>)\n",
    "            RETURNS <return_type>\n",
    "            LANGUAGE <language>\n",
    "            PACKAGES = (<packages>)\n",
    "            runtime_version = <runtime>\n",
    "            HANDLER = '<name>'\n",
    "            AS \n",
    "        $$\n",
    "        <def>\n",
    "        $$;\",\n",
    "    }\n",
    "    PROPS {\n",
    "        \"args\": {\n",
    "                \"propType\": \"map\",\n",
    "                \"format\": \"<key> <value>\",\n",
    "                \"delimiter\": \", \"\n",
    "            },\n",
    "        \"packages\": {\"propType\": \"list\", \"format\": \"'<item>'\", \"delimiter\": \", \"}\n",
    "    }\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b019448-2b04-4063-a812-bb598122ef04",
   "metadata": {},
   "source": [
    "### List objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b13db8f-0e7c-4e0b-8b88-ba092fe91530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalog.sql(\n",
    "    \"\"\"\n",
    "    SHOW OPEN TYPES\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "09d6a8bb-2989-4873-a8b4-e78416aea85f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>props</th>\n",
       "      <th>createdTimestamp</th>\n",
       "      <th>lastUpdatedTimestamp</th>\n",
       "      <th>entityVersion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>function_v2</td>\n",
       "      <td>foo</td>\n",
       "      <td>{'args': {'arg1': 'int', 'arg2': 'int'}, 'return_type': 'int', 'def': 'def foo(arg1, arg2):\n",
       "\n",
       "        return arg1 + arg2', 'signature': 'foo(arg1 str, arg2 int)', 'runtime': '3.12', 'language': 'python', 'comment': 'test fun', 'packages': ['numpy', 'pandas'], 'client_version': 1}</td>\n",
       "      <td>2025-06-07T14:52:45.212726551Z</td>\n",
       "      <td>2025-06-07T14:52:45.212734967Z</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          type name                                                                                                                                                                                                                                                                                    props                createdTimestamp            lastUpdatedTimestamp  entityVersion\n",
       "0  function_v2  foo  {'args': {'arg1': 'int', 'arg2': 'int'}, 'return_type': 'int', 'def': 'def foo(arg1, arg2):\n",
       "\n",
       "        return arg1 + arg2', 'signature': 'foo(arg1 str, arg2 int)', 'runtime': '3.12', 'language': 'python', 'comment': 'test fun', 'packages': ['numpy', 'pandas'], 'client_version': 1}  2025-06-07T14:52:45.212726551Z  2025-06-07T14:52:45.212734967Z              1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalog.sql(\n",
    "    \"\"\"\n",
    "    SHOW OPEN function_v2\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08a0829-e1bc-40a4-b2e9-583369bc2785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show mapping for <object> to <platform>. Example: [Platform_mapping(function_v2 -> snowflake)]\n",
    "catalog.sql(\n",
    "    \"\"\"\n",
    "    SHOW OPEN MAPPING function_v2 PLATFORM snowflake\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465f2f0d-4042-4304-92e7-9b08b1154757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all mappings from <object>. Example: [snowflake,spark]\n",
    "catalog.sql(\n",
    "    \"\"\"\n",
    "    SHOW OPEN PLATFORMS FOR function_v2\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38ccc6c-fac8-4d6c-a9d6-a556d5a91037",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog.sql(\n",
    "    \"\"\"\n",
    "    SHOW OPEN PLATFORMS\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9d02df-c4f4-46f4-b707-99d3d2869d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog.sql(\n",
    "    \"\"\"\n",
    "    SHOW OPEN MAPPINGS FOR snowflake\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2991f5-6099-4bc0-9e64-dda501c6104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog.sql(\n",
    "    \"\"\"\n",
    "    SYNC OPEN function_v2 for snowflake\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18be6e2-30ca-44ae-af80-da349d61916d",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog.sql(\n",
    "    \"\"\"\n",
    "    SYNC OPEN OBJECTS for snowflake\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a13475c-abcc-4bed-96d5-0d9044aee319",
   "metadata": {},
   "source": [
    "### Drop andfunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ba6692-c3a3-45e8-b1a6-a7d33697a990",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog.sql(\n",
    "    \"\"\"\n",
    "    DROP OPEN function_v2\n",
    "    \"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159960df-32ec-4d43-a16a-86e32780e4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog.sql(\n",
    "    \"\"\"\n",
    "    DROP OPEN MAPPINGS for snowflake\n",
    "    \"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f67035-aa7b-4eab-8ba0-7e7132c68106",
   "metadata": {},
   "source": [
    "### Visualize opendic tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66730eb2-aa42-42b5-8f15-f6fb6c3908d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>namespace</th>\n",
       "            <th>tableName</th>\n",
       "            <th>isTemporary</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>SYSTEM</td>\n",
       "            <td>function_v2</td>\n",
       "            <td>False</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+-----------+-------------+-------------+\n",
       "| namespace |   tableName | isTemporary |\n",
       "+-----------+-------------+-------------+\n",
       "|    SYSTEM | function_v2 |       False |\n",
       "+-----------+-------------+-------------+"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "show tables in SYSTEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ef1708a-3517-4c09-aaad-f51d08929cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>name</th>\n",
       "            <th>args</th>\n",
       "            <th>return_type</th>\n",
       "            <th>signature</th>\n",
       "            <th>runtime</th>\n",
       "            <th>language</th>\n",
       "            <th>comment</th>\n",
       "            <th>packages</th>\n",
       "            <th>def</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>foo</td>\n",
       "            <td>{&#x27;arg2&#x27;: &#x27;int&#x27;, &#x27;arg1&#x27;: &#x27;int&#x27;}</td>\n",
       "            <td>int</td>\n",
       "            <td>foo(arg1 str, arg2 int)</td>\n",
       "            <td>3.12</td>\n",
       "            <td>python</td>\n",
       "            <td>test fun</td>\n",
       "            <td>[&#x27;numpy&#x27;, &#x27;pandas&#x27;]</td>\n",
       "            <td>def foo(arg1, arg2):<br><br>        return arg1 + arg2</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+------+--------------------------------+-------------+-------------------------+---------+----------+----------+---------------------+----------------------------+\n",
       "| name |                           args | return_type |               signature | runtime | language |  comment |            packages |                        def |\n",
       "+------+--------------------------------+-------------+-------------------------+---------+----------+----------+---------------------+----------------------------+\n",
       "|  foo | {'arg2': 'int', 'arg1': 'int'} |         int | foo(arg1 str, arg2 int) |    3.12 |   python | test fun | ['numpy', 'pandas'] |       def foo(arg1, arg2): |\n",
       "|      |                                |             |                         |         |          |          |                     |                            |\n",
       "|      |                                |             |                         |         |          |          |                     |         return arg1 + arg2 |\n",
       "+------+--------------------------------+-------------+-------------------------+---------+----------+----------+---------------------+----------------------------+"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select uname as name, args, return_type, signature, runtime, language, comment, packages, def   from SYSTEM.function_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1dbb219f-304b-47c3-91b1-6bf220580183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "USE SYSTEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2365f5c0-5d66-4330-8602-f665a5ffee82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>uname</th>\n",
       "            <th>args</th>\n",
       "            <th>return_type</th>\n",
       "            <th>def</th>\n",
       "            <th>signature</th>\n",
       "            <th>runtime</th>\n",
       "            <th>language</th>\n",
       "            <th>comment</th>\n",
       "            <th>packages</th>\n",
       "            <th>client_version</th>\n",
       "            <th>created_time</th>\n",
       "            <th>last_updated_time</th>\n",
       "            <th>entity_version</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>foo</td>\n",
       "            <td>{&#x27;arg2&#x27;: &#x27;int&#x27;, &#x27;arg1&#x27;: &#x27;int&#x27;}</td>\n",
       "            <td>int</td>\n",
       "            <td>def foo(arg1, arg2):<br><br>        return arg1 + arg2</td>\n",
       "            <td>foo(arg1 str, arg2 int)</td>\n",
       "            <td>3.12</td>\n",
       "            <td>python</td>\n",
       "            <td>test fun</td>\n",
       "            <td>[&#x27;numpy&#x27;, &#x27;pandas&#x27;]</td>\n",
       "            <td>1</td>\n",
       "            <td>2025-06-08T17:58:27.161420711Z</td>\n",
       "            <td>2025-06-08T17:58:27.161423271Z</td>\n",
       "            <td>1</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+-------+--------------------------------+-------------+----------------------------+-------------------------+---------+----------+----------+---------------------+----------------+--------------------------------+--------------------------------+----------------+\n",
       "| uname |                           args | return_type |                        def |               signature | runtime | language |  comment |            packages | client_version |                   created_time |              last_updated_time | entity_version |\n",
       "+-------+--------------------------------+-------------+----------------------------+-------------------------+---------+----------+----------+---------------------+----------------+--------------------------------+--------------------------------+----------------+\n",
       "|   foo | {'arg2': 'int', 'arg1': 'int'} |         int |       def foo(arg1, arg2): | foo(arg1 str, arg2 int) |    3.12 |   python | test fun | ['numpy', 'pandas'] |              1 | 2025-06-08T17:58:27.161420711Z | 2025-06-08T17:58:27.161423271Z |              1 |\n",
       "|       |                                |             |                            |                         |         |          |          |                     |                |                                |                                |                |\n",
       "|       |                                |             |         return arg1 + arg2 |                         |         |          |          |                     |                |                                |                                |                |\n",
       "+-------+--------------------------------+-------------+----------------------------+-------------------------+---------+----------+----------+---------------------+----------------+--------------------------------+--------------------------------+----------------+"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select * from SYSTEM.function_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9a9f41",
   "metadata": {},
   "source": [
    "## Load One Month of NYC Taxi/Limousine Trip Data\n",
    "\n",
    "For this notebook, we will use the New York City Taxi and Limousine Commision Trip Record Data that's available on the AWS Open Data Registry. This contains data of trips taken by taxis and for-hire vehicles in New York City. We'll save this into an iceberg table called `taxis`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747bee98",
   "metadata": {},
   "source": [
    "To be able to rerun the notebook several times, let's drop the table if it exists to start fresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "930682ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/08 17:56:53 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "25/06/08 17:56:53 WARN RESTSessionCatalog: Iceberg REST client is missing the OAuth2 server URI configuration and defaults to https://opendict.duckdns.org/api/catalog/v1/oauth/tokens. This automatic fallback will be removed in a future Iceberg release.It is recommended to configure the OAuth2 endpoint using the 'oauth2-server-uri' property to be prepared. This warning will disappear if the OAuth2 endpoint is explicitly configured. See https://github.com/apache/iceberg/issues/10537\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE DATABASE IF NOT EXISTS nyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f918310a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "DROP TABLE IF EXISTS nyc.taxis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c37ca92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/08 17:56:58 WARN ADLSFileIO: Failed to delete path: abfss://polarisbucket@polarisstorageacc.blob.core.windows.net/warehouse/nyc/taxis/data/00000-1-f18c8544-c33f-4f11-b870-626609e33420-0-00001.parquet\n",
      "com.azure.storage.file.datalake.models.DataLakeStorageException: Status code 404, \"{\"error\":{\"code\":\"PathNotFound\",\"message\":\"The specified path does not exist.\\nRequestId:1efb88b8-e01f-0000-6d9e-d8e251000000\\nTime:2025-06-08T17:56:58.4021615Z\"}}\"\n",
      "\tat com.azure.storage.file.datalake.implementation.util.ModelHelper.mapToDataLakeStorageException(ModelHelper.java:210)\n",
      "\tat com.azure.storage.file.datalake.implementation.PathsImpl.deleteWithResponse(PathsImpl.java:5813)\n",
      "\tat com.azure.storage.file.datalake.DataLakePathClient.lambda$deleteWithResponse$1(DataLakePathClient.java:643)\n",
      "\tat com.azure.storage.common.implementation.StorageImplUtils.sendRequest(StorageImplUtils.java:494)\n",
      "\tat com.azure.storage.file.datalake.DataLakePathClient.deleteWithResponse(DataLakePathClient.java:651)\n",
      "\tat com.azure.storage.file.datalake.DataLakeFileClient.deleteWithResponse(DataLakeFileClient.java:224)\n",
      "\tat com.azure.storage.file.datalake.DataLakeFileClient.delete(DataLakeFileClient.java:192)\n",
      "\tat org.apache.iceberg.azure.adlsv2.ADLSFileIO.deleteFile(ADLSFileIO.java:92)\n",
      "\tat org.apache.iceberg.io.FileIO.deleteFile(FileIO.java:86)\n",
      "\tat org.apache.iceberg.io.RollingFileWriter.closeCurrentWriter(RollingFileWriter.java:129)\n",
      "\tat org.apache.iceberg.io.RollingFileWriter.close(RollingFileWriter.java:147)\n",
      "\tat org.apache.iceberg.io.RollingDataWriter.close(RollingDataWriter.java:32)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.close(SparkWrite.java:747)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.commit(SparkWrite.java:729)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:475)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/06/08 17:56:58 WARN ADLSFileIO: Failed to delete path: abfss://polarisbucket@polarisstorageacc.blob.core.windows.net/warehouse/nyc/taxis/data/00003-4-f18c8544-c33f-4f11-b870-626609e33420-0-00001.parquet\n",
      "com.azure.storage.file.datalake.models.DataLakeStorageException: Status code 404, \"{\"error\":{\"code\":\"PathNotFound\",\"message\":\"The specified path does not exist.\\nRequestId:5a7b1278-601f-001e-6c9e-d80e89000000\\nTime:2025-06-08T17:56:58.4082196Z\"}}\"\n",
      "\tat com.azure.storage.file.datalake.implementation.util.ModelHelper.mapToDataLakeStorageException(ModelHelper.java:210)\n",
      "\tat com.azure.storage.file.datalake.implementation.PathsImpl.deleteWithResponse(PathsImpl.java:5813)\n",
      "\tat com.azure.storage.file.datalake.DataLakePathClient.lambda$deleteWithResponse$1(DataLakePathClient.java:643)\n",
      "\tat com.azure.storage.common.implementation.StorageImplUtils.sendRequest(StorageImplUtils.java:494)\n",
      "\tat com.azure.storage.file.datalake.DataLakePathClient.deleteWithResponse(DataLakePathClient.java:651)\n",
      "\tat com.azure.storage.file.datalake.DataLakeFileClient.deleteWithResponse(DataLakeFileClient.java:224)\n",
      "\tat com.azure.storage.file.datalake.DataLakeFileClient.delete(DataLakeFileClient.java:192)\n",
      "\tat org.apache.iceberg.azure.adlsv2.ADLSFileIO.deleteFile(ADLSFileIO.java:92)\n",
      "\tat org.apache.iceberg.io.FileIO.deleteFile(FileIO.java:86)\n",
      "\tat org.apache.iceberg.io.RollingFileWriter.closeCurrentWriter(RollingFileWriter.java:129)\n",
      "\tat org.apache.iceberg.io.RollingFileWriter.close(RollingFileWriter.java:147)\n",
      "\tat org.apache.iceberg.io.RollingDataWriter.close(RollingDataWriter.java:32)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.close(SparkWrite.java:747)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.commit(SparkWrite.java:729)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:475)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/06/08 17:56:58 WARN ADLSFileIO: Failed to delete path: abfss://polarisbucket@polarisstorageacc.blob.core.windows.net/warehouse/nyc/taxis/data/00006-7-f18c8544-c33f-4f11-b870-626609e33420-0-00001.parquet\n",
      "com.azure.storage.file.datalake.models.DataLakeStorageException: Status code 404, \"{\"error\":{\"code\":\"PathNotFound\",\"message\":\"The specified path does not exist.\\nRequestId:7aa164bc-801f-004b-039e-d81e02000000\\nTime:2025-06-08T17:56:58.4030750Z\"}}\"\n",
      "\tat com.azure.storage.file.datalake.implementation.util.ModelHelper.mapToDataLakeStorageException(ModelHelper.java:210)\n",
      "\tat com.azure.storage.file.datalake.implementation.PathsImpl.deleteWithResponse(PathsImpl.java:5813)\n",
      "\tat com.azure.storage.file.datalake.DataLakePathClient.lambda$deleteWithResponse$1(DataLakePathClient.java:643)\n",
      "\tat com.azure.storage.common.implementation.StorageImplUtils.sendRequest(StorageImplUtils.java:494)\n",
      "\tat com.azure.storage.file.datalake.DataLakePathClient.deleteWithResponse(DataLakePathClient.java:651)\n",
      "\tat com.azure.storage.file.datalake.DataLakeFileClient.deleteWithResponse(DataLakeFileClient.java:224)\n",
      "\tat com.azure.storage.file.datalake.DataLakeFileClient.delete(DataLakeFileClient.java:192)\n",
      "\tat org.apache.iceberg.azure.adlsv2.ADLSFileIO.deleteFile(ADLSFileIO.java:92)\n",
      "\tat org.apache.iceberg.io.FileIO.deleteFile(FileIO.java:86)\n",
      "\tat org.apache.iceberg.io.RollingFileWriter.closeCurrentWriter(RollingFileWriter.java:129)\n",
      "\tat org.apache.iceberg.io.RollingFileWriter.close(RollingFileWriter.java:147)\n",
      "\tat org.apache.iceberg.io.RollingDataWriter.close(RollingDataWriter.java:32)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.close(SparkWrite.java:747)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.commit(SparkWrite.java:729)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:475)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/06/08 17:56:58 WARN ADLSFileIO: Failed to delete path: abfss://polarisbucket@polarisstorageacc.blob.core.windows.net/warehouse/nyc/taxis/data/00008-9-f18c8544-c33f-4f11-b870-626609e33420-0-00001.parquet\n",
      "com.azure.storage.file.datalake.models.DataLakeStorageException: Status code 404, \"{\"error\":{\"code\":\"PathNotFound\",\"message\":\"The specified path does not exist.\\nRequestId:af9b814e-c01f-0075-359e-d8897d000000\\nTime:2025-06-08T17:56:58.3933782Z\"}}\"\n",
      "\tat com.azure.storage.file.datalake.implementation.util.ModelHelper.mapToDataLakeStorageException(ModelHelper.java:210)\n",
      "\tat com.azure.storage.file.datalake.implementation.PathsImpl.deleteWithResponse(PathsImpl.java:5813)\n",
      "\tat com.azure.storage.file.datalake.DataLakePathClient.lambda$deleteWithResponse$1(DataLakePathClient.java:643)\n",
      "\tat com.azure.storage.common.implementation.StorageImplUtils.sendRequest(StorageImplUtils.java:494)\n",
      "\tat com.azure.storage.file.datalake.DataLakePathClient.deleteWithResponse(DataLakePathClient.java:651)\n",
      "\tat com.azure.storage.file.datalake.DataLakeFileClient.deleteWithResponse(DataLakeFileClient.java:224)\n",
      "\tat com.azure.storage.file.datalake.DataLakeFileClient.delete(DataLakeFileClient.java:192)\n",
      "\tat org.apache.iceberg.azure.adlsv2.ADLSFileIO.deleteFile(ADLSFileIO.java:92)\n",
      "\tat org.apache.iceberg.io.FileIO.deleteFile(FileIO.java:86)\n",
      "\tat org.apache.iceberg.io.RollingFileWriter.closeCurrentWriter(RollingFileWriter.java:129)\n",
      "\tat org.apache.iceberg.io.RollingFileWriter.close(RollingFileWriter.java:147)\n",
      "\tat org.apache.iceberg.io.RollingDataWriter.close(RollingDataWriter.java:32)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.close(SparkWrite.java:747)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.commit(SparkWrite.java:729)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:475)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/06/08 17:56:58 WARN ADLSFileIO: Failed to delete path: abfss://polarisbucket@polarisstorageacc.blob.core.windows.net/warehouse/nyc/taxis/data/00007-8-f18c8544-c33f-4f11-b870-626609e33420-0-00001.parquet\n",
      "com.azure.storage.file.datalake.models.DataLakeStorageException: Status code 404, \"{\"error\":{\"code\":\"PathNotFound\",\"message\":\"The specified path does not exist.\\nRequestId:8e92271b-401f-0019-1a9e-d862ea000000\\nTime:2025-06-08T17:56:58.4043024Z\"}}\"\n",
      "\tat com.azure.storage.file.datalake.implementation.util.ModelHelper.mapToDataLakeStorageException(ModelHelper.java:210)\n",
      "\tat com.azure.storage.file.datalake.implementation.PathsImpl.deleteWithResponse(PathsImpl.java:5813)\n",
      "\tat com.azure.storage.file.datalake.DataLakePathClient.lambda$deleteWithResponse$1(DataLakePathClient.java:643)\n",
      "\tat com.azure.storage.common.implementation.StorageImplUtils.sendRequest(StorageImplUtils.java:494)\n",
      "\tat com.azure.storage.file.datalake.DataLakePathClient.deleteWithResponse(DataLakePathClient.java:651)\n",
      "\tat com.azure.storage.file.datalake.DataLakeFileClient.deleteWithResponse(DataLakeFileClient.java:224)\n",
      "\tat com.azure.storage.file.datalake.DataLakeFileClient.delete(DataLakeFileClient.java:192)\n",
      "\tat org.apache.iceberg.azure.adlsv2.ADLSFileIO.deleteFile(ADLSFileIO.java:92)\n",
      "\tat org.apache.iceberg.io.FileIO.deleteFile(FileIO.java:86)\n",
      "\tat org.apache.iceberg.io.RollingFileWriter.closeCurrentWriter(RollingFileWriter.java:129)\n",
      "\tat org.apache.iceberg.io.RollingFileWriter.close(RollingFileWriter.java:147)\n",
      "\tat org.apache.iceberg.io.RollingDataWriter.close(RollingDataWriter.java:32)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.close(SparkWrite.java:747)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.commit(SparkWrite.java:729)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:475)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/06/08 17:56:58 WARN ADLSFileIO: Failed to delete path: abfss://polarisbucket@polarisstorageacc.blob.core.windows.net/warehouse/nyc/taxis/data/00005-6-f18c8544-c33f-4f11-b870-626609e33420-0-00001.parquet\n",
      "com.azure.storage.file.datalake.models.DataLakeStorageException: Status code 404, \"{\"error\":{\"code\":\"PathNotFound\",\"message\":\"The specified path does not exist.\\nRequestId:19d49a17-c01f-004a-719e-d841de000000\\nTime:2025-06-08T17:56:58.3965948Z\"}}\"\n",
      "\tat com.azure.storage.file.datalake.implementation.util.ModelHelper.mapToDataLakeStorageException(ModelHelper.java:210)\n",
      "\tat com.azure.storage.file.datalake.implementation.PathsImpl.deleteWithResponse(PathsImpl.java:5813)\n",
      "\tat com.azure.storage.file.datalake.DataLakePathClient.lambda$deleteWithResponse$1(DataLakePathClient.java:643)\n",
      "\tat com.azure.storage.common.implementation.StorageImplUtils.sendRequest(StorageImplUtils.java:494)\n",
      "\tat com.azure.storage.file.datalake.DataLakePathClient.deleteWithResponse(DataLakePathClient.java:651)\n",
      "\tat com.azure.storage.file.datalake.DataLakeFileClient.deleteWithResponse(DataLakeFileClient.java:224)\n",
      "\tat com.azure.storage.file.datalake.DataLakeFileClient.delete(DataLakeFileClient.java:192)\n",
      "\tat org.apache.iceberg.azure.adlsv2.ADLSFileIO.deleteFile(ADLSFileIO.java:92)\n",
      "\tat org.apache.iceberg.io.FileIO.deleteFile(FileIO.java:86)\n",
      "\tat org.apache.iceberg.io.RollingFileWriter.closeCurrentWriter(RollingFileWriter.java:129)\n",
      "\tat org.apache.iceberg.io.RollingFileWriter.close(RollingFileWriter.java:147)\n",
      "\tat org.apache.iceberg.io.RollingDataWriter.close(RollingDataWriter.java:32)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.close(SparkWrite.java:747)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.commit(SparkWrite.java:729)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:475)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/06/08 17:56:58 WARN ADLSFileIO: Failed to delete path: abfss://polarisbucket@polarisstorageacc.blob.core.windows.net/warehouse/nyc/taxis/data/00001-2-f18c8544-c33f-4f11-b870-626609e33420-0-00001.parquet\n",
      "com.azure.storage.file.datalake.models.DataLakeStorageException: Status code 404, \"{\"error\":{\"code\":\"PathNotFound\",\"message\":\"The specified path does not exist.\\nRequestId:2b1ad177-001f-0027-2f9e-d8f595000000\\nTime:2025-06-08T17:56:58.4104651Z\"}}\"\n",
      "\tat com.azure.storage.file.datalake.implementation.util.ModelHelper.mapToDataLakeStorageException(ModelHelper.java:210)\n",
      "\tat com.azure.storage.file.datalake.implementation.PathsImpl.deleteWithResponse(PathsImpl.java:5813)\n",
      "\tat com.azure.storage.file.datalake.DataLakePathClient.lambda$deleteWithResponse$1(DataLakePathClient.java:643)\n",
      "\tat com.azure.storage.common.implementation.StorageImplUtils.sendRequest(StorageImplUtils.java:494)\n",
      "\tat com.azure.storage.file.datalake.DataLakePathClient.deleteWithResponse(DataLakePathClient.java:651)\n",
      "\tat com.azure.storage.file.datalake.DataLakeFileClient.deleteWithResponse(DataLakeFileClient.java:224)\n",
      "\tat com.azure.storage.file.datalake.DataLakeFileClient.delete(DataLakeFileClient.java:192)\n",
      "\tat org.apache.iceberg.azure.adlsv2.ADLSFileIO.deleteFile(ADLSFileIO.java:92)\n",
      "\tat org.apache.iceberg.io.FileIO.deleteFile(FileIO.java:86)\n",
      "\tat org.apache.iceberg.io.RollingFileWriter.closeCurrentWriter(RollingFileWriter.java:129)\n",
      "\tat org.apache.iceberg.io.RollingFileWriter.close(RollingFileWriter.java:147)\n",
      "\tat org.apache.iceberg.io.RollingDataWriter.close(RollingDataWriter.java:32)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.close(SparkWrite.java:747)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.commit(SparkWrite.java:729)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:475)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/06/08 17:56:58 WARN ADLSFileIO: Failed to delete path: abfss://polarisbucket@polarisstorageacc.blob.core.windows.net/warehouse/nyc/taxis/data/00002-3-f18c8544-c33f-4f11-b870-626609e33420-0-00001.parquet\n",
      "com.azure.storage.file.datalake.models.DataLakeStorageException: Status code 404, \"{\"error\":{\"code\":\"PathNotFound\",\"message\":\"The specified path does not exist.\\nRequestId:a15e79c9-b01f-0022-1f9e-d8274e000000\\nTime:2025-06-08T17:56:58.4114858Z\"}}\"\n",
      "\tat com.azure.storage.file.datalake.implementation.util.ModelHelper.mapToDataLakeStorageException(ModelHelper.java:210)\n",
      "\tat com.azure.storage.file.datalake.implementation.PathsImpl.deleteWithResponse(PathsImpl.java:5813)\n",
      "\tat com.azure.storage.file.datalake.DataLakePathClient.lambda$deleteWithResponse$1(DataLakePathClient.java:643)\n",
      "\tat com.azure.storage.common.implementation.StorageImplUtils.sendRequest(StorageImplUtils.java:494)\n",
      "\tat com.azure.storage.file.datalake.DataLakePathClient.deleteWithResponse(DataLakePathClient.java:651)\n",
      "\tat com.azure.storage.file.datalake.DataLakeFileClient.deleteWithResponse(DataLakeFileClient.java:224)\n",
      "\tat com.azure.storage.file.datalake.DataLakeFileClient.delete(DataLakeFileClient.java:192)\n",
      "\tat org.apache.iceberg.azure.adlsv2.ADLSFileIO.deleteFile(ADLSFileIO.java:92)\n",
      "\tat org.apache.iceberg.io.FileIO.deleteFile(FileIO.java:86)\n",
      "\tat org.apache.iceberg.io.RollingFileWriter.closeCurrentWriter(RollingFileWriter.java:129)\n",
      "\tat org.apache.iceberg.io.RollingFileWriter.close(RollingFileWriter.java:147)\n",
      "\tat org.apache.iceberg.io.RollingDataWriter.close(RollingDataWriter.java:32)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.close(SparkWrite.java:747)\n",
      "\tat org.apache.iceberg.spark.source.SparkWrite$UnpartitionedDataWriter.commit(SparkWrite.java:729)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.$anonfun$run$5(WriteToDataSourceV2Exec.scala:475)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run(WriteToDataSourceV2Exec.scala:491)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WritingSparkTask.run$(WriteToDataSourceV2Exec.scala:430)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.DataWritingSparkTask$.run(WriteToDataSourceV2Exec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.$anonfun$writeWithV2$2(WriteToDataSourceV2Exec.scala:393)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"/home/iceberg/data/yellow_tripdata_2021-04.parquet\")\n",
    "df.write.saveAsTable(\"nyc.taxis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fddb808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>col_name</th>\n",
       "            <th>data_type</th>\n",
       "            <th>comment</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>VendorID</td>\n",
       "            <td>bigint</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>tpep_pickup_datetime</td>\n",
       "            <td>timestamp_ntz</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>tpep_dropoff_datetime</td>\n",
       "            <td>timestamp_ntz</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>passenger_count</td>\n",
       "            <td>double</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>trip_distance</td>\n",
       "            <td>double</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>RatecodeID</td>\n",
       "            <td>double</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>store_and_fwd_flag</td>\n",
       "            <td>string</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>PULocationID</td>\n",
       "            <td>bigint</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>DOLocationID</td>\n",
       "            <td>bigint</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>payment_type</td>\n",
       "            <td>bigint</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>fare_amount</td>\n",
       "            <td>double</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>extra</td>\n",
       "            <td>double</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>mta_tax</td>\n",
       "            <td>double</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>tip_amount</td>\n",
       "            <td>double</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>tolls_amount</td>\n",
       "            <td>double</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>improvement_surcharge</td>\n",
       "            <td>double</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>total_amount</td>\n",
       "            <td>double</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>congestion_surcharge</td>\n",
       "            <td>double</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>airport_fee</td>\n",
       "            <td>double</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td></td>\n",
       "            <td></td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td># Metadata Columns</td>\n",
       "            <td></td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>_spec_id</td>\n",
       "            <td>int</td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>_partition</td>\n",
       "            <td>struct&lt;&gt;</td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>_file</td>\n",
       "            <td>string</td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>_pos</td>\n",
       "            <td>bigint</td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>_deleted</td>\n",
       "            <td>boolean</td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td></td>\n",
       "            <td></td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td># Detailed Table Information</td>\n",
       "            <td></td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Name</td>\n",
       "            <td>polaris.nyc.taxis</td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Type</td>\n",
       "            <td>MANAGED</td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Location</td>\n",
       "            <td>abfss://polarisbucket@polarisstorageacc.blob.core.windows.net/warehouse/nyc/taxis</td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Provider</td>\n",
       "            <td>iceberg</td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Owner</td>\n",
       "            <td>root</td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Table Properties</td>\n",
       "            <td>[created-at=2025-06-08T17:56:57.351098653Z,current-snapshot-id=5110471473628533297,format=iceberg/parquet,format-version=2,write.format.default=parquet,write.parquet.compression-codec=zstd]</td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+\n",
       "|                     col_name |                                                                                                                                                                                     data_type | comment |\n",
       "+------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+\n",
       "|                     VendorID |                                                                                                                                                                                        bigint |    None |\n",
       "|         tpep_pickup_datetime |                                                                                                                                                                                 timestamp_ntz |    None |\n",
       "|        tpep_dropoff_datetime |                                                                                                                                                                                 timestamp_ntz |    None |\n",
       "|              passenger_count |                                                                                                                                                                                        double |    None |\n",
       "|                trip_distance |                                                                                                                                                                                        double |    None |\n",
       "|                   RatecodeID |                                                                                                                                                                                        double |    None |\n",
       "|           store_and_fwd_flag |                                                                                                                                                                                        string |    None |\n",
       "|                 PULocationID |                                                                                                                                                                                        bigint |    None |\n",
       "|                 DOLocationID |                                                                                                                                                                                        bigint |    None |\n",
       "|                 payment_type |                                                                                                                                                                                        bigint |    None |\n",
       "|                  fare_amount |                                                                                                                                                                                        double |    None |\n",
       "|                        extra |                                                                                                                                                                                        double |    None |\n",
       "|                      mta_tax |                                                                                                                                                                                        double |    None |\n",
       "|                   tip_amount |                                                                                                                                                                                        double |    None |\n",
       "|                 tolls_amount |                                                                                                                                                                                        double |    None |\n",
       "|        improvement_surcharge |                                                                                                                                                                                        double |    None |\n",
       "|                 total_amount |                                                                                                                                                                                        double |    None |\n",
       "|         congestion_surcharge |                                                                                                                                                                                        double |    None |\n",
       "|                  airport_fee |                                                                                                                                                                                        double |    None |\n",
       "|                              |                                                                                                                                                                                               |         |\n",
       "|           # Metadata Columns |                                                                                                                                                                                               |         |\n",
       "|                     _spec_id |                                                                                                                                                                                           int |         |\n",
       "|                   _partition |                                                                                                                                                                                      struct<> |         |\n",
       "|                        _file |                                                                                                                                                                                        string |         |\n",
       "|                         _pos |                                                                                                                                                                                        bigint |         |\n",
       "|                     _deleted |                                                                                                                                                                                       boolean |         |\n",
       "|                              |                                                                                                                                                                                               |         |\n",
       "| # Detailed Table Information |                                                                                                                                                                                               |         |\n",
       "|                         Name |                                                                                                                                                                             polaris.nyc.taxis |         |\n",
       "|                         Type |                                                                                                                                                                                       MANAGED |         |\n",
       "|                     Location |                                                                                                             abfss://polarisbucket@polarisstorageacc.blob.core.windows.net/warehouse/nyc/taxis |         |\n",
       "|                     Provider |                                                                                                                                                                                       iceberg |         |\n",
       "|                        Owner |                                                                                                                                                                                          root |         |\n",
       "|             Table Properties | [created-at=2025-06-08T17:56:57.351098653Z,current-snapshot-id=5110471473628533297,format=iceberg/parquet,format-version=2,write.format.default=parquet,write.parquet.compression-codec=zstd] |         |\n",
       "+------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "DESCRIBE EXTENDED nyc.taxis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcf99fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>cnt</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>2171187</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+---------+\n",
       "|     cnt |\n",
       "+---------+\n",
       "| 2171187 |\n",
       "+---------+"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT COUNT(*) as cnt\n",
    "FROM nyc.taxis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "602cb02b-b5ee-4a2f-be39-030671dcb509",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>VendorID</th>\n",
       "            <th>tpep_pickup_datetime</th>\n",
       "            <th>tpep_dropoff_datetime</th>\n",
       "            <th>passenger_count</th>\n",
       "            <th>trip_distance</th>\n",
       "            <th>RatecodeID</th>\n",
       "            <th>store_and_fwd_flag</th>\n",
       "            <th>PULocationID</th>\n",
       "            <th>DOLocationID</th>\n",
       "            <th>payment_type</th>\n",
       "            <th>fare_amount</th>\n",
       "            <th>extra</th>\n",
       "            <th>mta_tax</th>\n",
       "            <th>tip_amount</th>\n",
       "            <th>tolls_amount</th>\n",
       "            <th>improvement_surcharge</th>\n",
       "            <th>total_amount</th>\n",
       "            <th>congestion_surcharge</th>\n",
       "            <th>airport_fee</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>1</td>\n",
       "            <td>2021-04-01 00:00:18</td>\n",
       "            <td>2021-04-01 00:21:54</td>\n",
       "            <td>1.0</td>\n",
       "            <td>8.4</td>\n",
       "            <td>1.0</td>\n",
       "            <td>N</td>\n",
       "            <td>79</td>\n",
       "            <td>116</td>\n",
       "            <td>1</td>\n",
       "            <td>25.5</td>\n",
       "            <td>3.0</td>\n",
       "            <td>0.5</td>\n",
       "            <td>5.85</td>\n",
       "            <td>0.0</td>\n",
       "            <td>0.3</td>\n",
       "            <td>35.15</td>\n",
       "            <td>2.5</td>\n",
       "            <td>0.0</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1</td>\n",
       "            <td>2021-04-01 00:42:37</td>\n",
       "            <td>2021-04-01 00:46:23</td>\n",
       "            <td>1.0</td>\n",
       "            <td>0.9</td>\n",
       "            <td>1.0</td>\n",
       "            <td>N</td>\n",
       "            <td>75</td>\n",
       "            <td>236</td>\n",
       "            <td>2</td>\n",
       "            <td>5.0</td>\n",
       "            <td>3.0</td>\n",
       "            <td>0.5</td>\n",
       "            <td>0.0</td>\n",
       "            <td>0.0</td>\n",
       "            <td>0.3</td>\n",
       "            <td>8.8</td>\n",
       "            <td>2.5</td>\n",
       "            <td>0.0</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1</td>\n",
       "            <td>2021-04-01 00:57:56</td>\n",
       "            <td>2021-04-01 01:08:22</td>\n",
       "            <td>1.0</td>\n",
       "            <td>3.4</td>\n",
       "            <td>1.0</td>\n",
       "            <td>N</td>\n",
       "            <td>236</td>\n",
       "            <td>168</td>\n",
       "            <td>2</td>\n",
       "            <td>11.5</td>\n",
       "            <td>3.0</td>\n",
       "            <td>0.5</td>\n",
       "            <td>0.0</td>\n",
       "            <td>0.0</td>\n",
       "            <td>0.3</td>\n",
       "            <td>15.3</td>\n",
       "            <td>2.5</td>\n",
       "            <td>0.0</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1</td>\n",
       "            <td>2021-04-01 00:01:58</td>\n",
       "            <td>2021-04-01 00:54:27</td>\n",
       "            <td>1.0</td>\n",
       "            <td>0.0</td>\n",
       "            <td>1.0</td>\n",
       "            <td>N</td>\n",
       "            <td>47</td>\n",
       "            <td>61</td>\n",
       "            <td>1</td>\n",
       "            <td>44.2</td>\n",
       "            <td>0.0</td>\n",
       "            <td>0.5</td>\n",
       "            <td>0.0</td>\n",
       "            <td>0.0</td>\n",
       "            <td>0.3</td>\n",
       "            <td>45.0</td>\n",
       "            <td>0.0</td>\n",
       "            <td>0.0</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2</td>\n",
       "            <td>2021-04-01 00:24:55</td>\n",
       "            <td>2021-04-01 00:34:33</td>\n",
       "            <td>1.0</td>\n",
       "            <td>1.96</td>\n",
       "            <td>1.0</td>\n",
       "            <td>N</td>\n",
       "            <td>238</td>\n",
       "            <td>152</td>\n",
       "            <td>1</td>\n",
       "            <td>9.0</td>\n",
       "            <td>0.5</td>\n",
       "            <td>0.5</td>\n",
       "            <td>3.09</td>\n",
       "            <td>0.0</td>\n",
       "            <td>0.3</td>\n",
       "            <td>13.39</td>\n",
       "            <td>0.0</td>\n",
       "            <td>0.0</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2</td>\n",
       "            <td>2021-04-01 00:19:16</td>\n",
       "            <td>2021-04-01 00:21:46</td>\n",
       "            <td>1.0</td>\n",
       "            <td>0.77</td>\n",
       "            <td>1.0</td>\n",
       "            <td>N</td>\n",
       "            <td>142</td>\n",
       "            <td>238</td>\n",
       "            <td>1</td>\n",
       "            <td>4.5</td>\n",
       "            <td>0.5</td>\n",
       "            <td>0.5</td>\n",
       "            <td>1.24</td>\n",
       "            <td>0.0</td>\n",
       "            <td>0.3</td>\n",
       "            <td>9.54</td>\n",
       "            <td>2.5</td>\n",
       "            <td>0.0</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2</td>\n",
       "            <td>2021-04-01 00:25:11</td>\n",
       "            <td>2021-04-01 00:31:53</td>\n",
       "            <td>1.0</td>\n",
       "            <td>3.65</td>\n",
       "            <td>1.0</td>\n",
       "            <td>N</td>\n",
       "            <td>238</td>\n",
       "            <td>244</td>\n",
       "            <td>1</td>\n",
       "            <td>11.5</td>\n",
       "            <td>0.5</td>\n",
       "            <td>0.5</td>\n",
       "            <td>2.56</td>\n",
       "            <td>0.0</td>\n",
       "            <td>0.3</td>\n",
       "            <td>15.36</td>\n",
       "            <td>0.0</td>\n",
       "            <td>0.0</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1</td>\n",
       "            <td>2021-04-01 00:27:53</td>\n",
       "            <td>2021-04-01 00:47:03</td>\n",
       "            <td>0.0</td>\n",
       "            <td>8.9</td>\n",
       "            <td>1.0</td>\n",
       "            <td>N</td>\n",
       "            <td>138</td>\n",
       "            <td>239</td>\n",
       "            <td>1</td>\n",
       "            <td>26.5</td>\n",
       "            <td>3.0</td>\n",
       "            <td>0.5</td>\n",
       "            <td>7.25</td>\n",
       "            <td>6.12</td>\n",
       "            <td>0.3</td>\n",
       "            <td>43.67</td>\n",
       "            <td>2.5</td>\n",
       "            <td>0.0</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>2</td>\n",
       "            <td>2021-04-01 00:24:24</td>\n",
       "            <td>2021-04-01 00:37:50</td>\n",
       "            <td>1.0</td>\n",
       "            <td>2.98</td>\n",
       "            <td>1.0</td>\n",
       "            <td>N</td>\n",
       "            <td>151</td>\n",
       "            <td>244</td>\n",
       "            <td>2</td>\n",
       "            <td>12.0</td>\n",
       "            <td>0.5</td>\n",
       "            <td>0.5</td>\n",
       "            <td>0.0</td>\n",
       "            <td>0.0</td>\n",
       "            <td>0.3</td>\n",
       "            <td>13.3</td>\n",
       "            <td>0.0</td>\n",
       "            <td>0.0</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>1</td>\n",
       "            <td>2021-04-01 00:19:18</td>\n",
       "            <td>2021-04-01 00:41:25</td>\n",
       "            <td>1.0</td>\n",
       "            <td>8.9</td>\n",
       "            <td>1.0</td>\n",
       "            <td>N</td>\n",
       "            <td>132</td>\n",
       "            <td>196</td>\n",
       "            <td>2</td>\n",
       "            <td>28.0</td>\n",
       "            <td>0.5</td>\n",
       "            <td>0.5</td>\n",
       "            <td>0.0</td>\n",
       "            <td>0.0</td>\n",
       "            <td>0.3</td>\n",
       "            <td>29.3</td>\n",
       "            <td>0.0</td>\n",
       "            <td>0.0</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+----------+----------------------+-----------------------+-----------------+---------------+------------+--------------------+--------------+--------------+--------------+-------------+-------+---------+------------+--------------+-----------------------+--------------+----------------------+-------------+\n",
       "| VendorID | tpep_pickup_datetime | tpep_dropoff_datetime | passenger_count | trip_distance | RatecodeID | store_and_fwd_flag | PULocationID | DOLocationID | payment_type | fare_amount | extra | mta_tax | tip_amount | tolls_amount | improvement_surcharge | total_amount | congestion_surcharge | airport_fee |\n",
       "+----------+----------------------+-----------------------+-----------------+---------------+------------+--------------------+--------------+--------------+--------------+-------------+-------+---------+------------+--------------+-----------------------+--------------+----------------------+-------------+\n",
       "|        1 |  2021-04-01 00:00:18 |   2021-04-01 00:21:54 |             1.0 |           8.4 |        1.0 |                  N |           79 |          116 |            1 |        25.5 |   3.0 |     0.5 |       5.85 |          0.0 |                   0.3 |        35.15 |                  2.5 |         0.0 |\n",
       "|        1 |  2021-04-01 00:42:37 |   2021-04-01 00:46:23 |             1.0 |           0.9 |        1.0 |                  N |           75 |          236 |            2 |         5.0 |   3.0 |     0.5 |        0.0 |          0.0 |                   0.3 |          8.8 |                  2.5 |         0.0 |\n",
       "|        1 |  2021-04-01 00:57:56 |   2021-04-01 01:08:22 |             1.0 |           3.4 |        1.0 |                  N |          236 |          168 |            2 |        11.5 |   3.0 |     0.5 |        0.0 |          0.0 |                   0.3 |         15.3 |                  2.5 |         0.0 |\n",
       "|        1 |  2021-04-01 00:01:58 |   2021-04-01 00:54:27 |             1.0 |           0.0 |        1.0 |                  N |           47 |           61 |            1 |        44.2 |   0.0 |     0.5 |        0.0 |          0.0 |                   0.3 |         45.0 |                  0.0 |         0.0 |\n",
       "|        2 |  2021-04-01 00:24:55 |   2021-04-01 00:34:33 |             1.0 |          1.96 |        1.0 |                  N |          238 |          152 |            1 |         9.0 |   0.5 |     0.5 |       3.09 |          0.0 |                   0.3 |        13.39 |                  0.0 |         0.0 |\n",
       "|        2 |  2021-04-01 00:19:16 |   2021-04-01 00:21:46 |             1.0 |          0.77 |        1.0 |                  N |          142 |          238 |            1 |         4.5 |   0.5 |     0.5 |       1.24 |          0.0 |                   0.3 |         9.54 |                  2.5 |         0.0 |\n",
       "|        2 |  2021-04-01 00:25:11 |   2021-04-01 00:31:53 |             1.0 |          3.65 |        1.0 |                  N |          238 |          244 |            1 |        11.5 |   0.5 |     0.5 |       2.56 |          0.0 |                   0.3 |        15.36 |                  0.0 |         0.0 |\n",
       "|        1 |  2021-04-01 00:27:53 |   2021-04-01 00:47:03 |             0.0 |           8.9 |        1.0 |                  N |          138 |          239 |            1 |        26.5 |   3.0 |     0.5 |       7.25 |         6.12 |                   0.3 |        43.67 |                  2.5 |         0.0 |\n",
       "|        2 |  2021-04-01 00:24:24 |   2021-04-01 00:37:50 |             1.0 |          2.98 |        1.0 |                  N |          151 |          244 |            2 |        12.0 |   0.5 |     0.5 |        0.0 |          0.0 |                   0.3 |         13.3 |                  0.0 |         0.0 |\n",
       "|        1 |  2021-04-01 00:19:18 |   2021-04-01 00:41:25 |             1.0 |           8.9 |        1.0 |                  N |          132 |          196 |            2 |        28.0 |   0.5 |     0.5 |        0.0 |          0.0 |                   0.3 |         29.3 |                  0.0 |         0.0 |\n",
       "+----------+----------------------+-----------------------+-----------------+---------------+------------+--------------------+--------------+--------------+--------------+-------------+-------+---------+------------+--------------+-----------------------+--------------+----------------------+-------------+"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT *\n",
    "FROM nyc.taxis limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffd2c03",
   "metadata": {},
   "source": [
    "## Schema Evolution\n",
    "\n",
    "Adding, dropping, renaming, or altering columns is easy and safe in Iceberg. In this example, we'll rename `fare_amount` to `fare` and `trip_distance` to `distance`. We'll also add a float column `fare_per_distance_unit` immediately after `distance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efee8252",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "ALTER TABLE nyc.taxis RENAME COLUMN fare_amount TO fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794de3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "ALTER TABLE nyc.taxis RENAME COLUMN trip_distance TO distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adac7564",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "ALTER TABLE nyc.taxis ALTER COLUMN distance COMMENT 'The elapsed trip distance in miles reported by the taximeter.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d7e6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "ALTER TABLE nyc.taxis ALTER COLUMN distance TYPE double;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb4b02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "ALTER TABLE nyc.taxis ALTER COLUMN distance AFTER fare;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f7cc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "ALTER TABLE nyc.taxis\n",
    "ADD COLUMN fare_per_distance_unit float AFTER distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9416b498",
   "metadata": {},
   "source": [
    "Let's update the new `fare_per_distance_unit` to equal `fare` divided by `distance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18771ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "UPDATE nyc.taxis\n",
    "SET fare_per_distance_unit = fare/distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c72ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT\n",
    "VendorID\n",
    ",tpep_pickup_datetime\n",
    ",tpep_dropoff_datetime\n",
    ",fare\n",
    ",distance\n",
    ",fare_per_distance_unit\n",
    "FROM nyc.taxis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37582e02",
   "metadata": {},
   "source": [
    "## Expressive SQL for Row Level Changes\n",
    "With Iceberg tables, `DELETE` queries can be used to perform row-level deletes. This is as simple as providing the table name and a `WHERE` predicate. If the filter matches an entire partition of the table, Iceberg will intelligently perform a metadata-only operation where it simply deletes the metadata for that partition.\n",
    "\n",
    "Let's perform a row-level delete for all rows that have a `fare_per_distance_unit` greater than 4 or a `distance` greater than 2. This should leave us with relatively short trips that have a relatively high fare per distance traveled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded820f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "DELETE FROM nyc.taxis\n",
    "WHERE fare_per_distance_unit > 4.0 OR distance > 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faef3712",
   "metadata": {},
   "source": [
    "There are some fares that have a `null` for `fare_per_distance_unit` due to the distance being `0`. Let's remove those as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b69265",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "DELETE FROM nyc.taxis\n",
    "WHERE fare_per_distance_unit is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b92d7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT\n",
    "VendorID\n",
    ",tpep_pickup_datetime\n",
    ",tpep_dropoff_datetime\n",
    ",fare\n",
    ",distance\n",
    ",fare_per_distance_unit\n",
    "FROM nyc.taxis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5472b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT COUNT(*) as cnt\n",
    "FROM nyc.taxis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b157e5",
   "metadata": {},
   "source": [
    "## Partitioning\n",
    "\n",
    "A table’s partitioning can be updated in place and applied only to newly written data. Query plans are then split, using the old partition scheme for data written before the partition scheme was changed, and using the new partition scheme for data written after. People querying the table don’t even have to be aware of this split. Simple predicates in WHERE clauses are automatically converted to partition filters that prune out files with no matches. This is what’s referred to in Iceberg as *Hidden Partitioning*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e3e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "ALTER TABLE nyc.taxis\n",
    "ADD PARTITION FIELD VendorID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fce6bb4",
   "metadata": {},
   "source": [
    "## Metadata Tables\n",
    "\n",
    "Iceberg tables contain very rich metadata that can be easily queried. For example, you can retrieve the manifest list for any snapshot, simply by querying the table's `snapshots` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fade1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT snapshot_id, manifest_list\n",
    "FROM nyc.taxis.snapshots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64887133",
   "metadata": {},
   "source": [
    "The `files` table contains loads of information on data files, including column level statistics such as null counts, lower bounds, and upper bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb712f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT file_path, file_format, record_count, null_value_counts, lower_bounds, upper_bounds\n",
    "FROM nyc.taxis.files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65deb074",
   "metadata": {},
   "source": [
    "## Time Travel\n",
    "\n",
    "The history table lists all snapshots and which parent snapshot they derive from. The `is_current_ancestor` flag let's you know if a snapshot is part of the linear history of the current snapshot of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab64f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT *\n",
    "FROM nyc.taxis.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47129d69",
   "metadata": {},
   "source": [
    "You can time-travel by altering the `current-snapshot-id` property of the table to reference any snapshot in the table's history. Let's revert the table to it's original state by traveling to the very first snapshot ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c360238",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql --var df\n",
    "\n",
    "SELECT *\n",
    "FROM nyc.taxis.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df43d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_snapshot = df.head().snapshot_id\n",
    "spark.sql(f\"CALL system.rollback_to_snapshot('nyc.taxis', {original_snapshot})\")\n",
    "original_snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955a4c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT\n",
    "VendorID\n",
    ",tpep_pickup_datetime\n",
    ",tpep_dropoff_datetime\n",
    ",fare\n",
    ",distance\n",
    ",fare_per_distance_unit\n",
    "FROM nyc.taxis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b71c76",
   "metadata": {},
   "source": [
    "Another look at the history table shows that the original state of the table has been added as a new entry\n",
    "with the original snapshot ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b801d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT *\n",
    "FROM nyc.taxis.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85667efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT COUNT(*) as cnt\n",
    "FROM nyc.taxis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
