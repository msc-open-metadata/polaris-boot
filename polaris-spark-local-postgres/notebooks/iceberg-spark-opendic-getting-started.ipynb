{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1041ae6f",
   "metadata": {},
   "source": [
    "![iceberg-logo](https://www.apache.org/logos/res/iceberg/iceberg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247fb2ab",
   "metadata": {},
   "source": [
    "### [Docker, Spark, and Iceberg: The Fastest Way to Try Iceberg!](https://tabular.io/blog/docker-spark-and-iceberg/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21411ff4-eeee-476d-ac2e-b2b727a1e2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark-opendic==0.3.17\n",
      "  Downloading pyspark_opendic-0.3.17-py3-none-any.whl (9.2 kB)\n",
      "Requirement already satisfied: pydantic>=2.10.6 in /usr/local/lib/python3.10/site-packages (from pyspark-opendic==0.3.17) (2.10.6)\n",
      "Requirement already satisfied: pyspark>=3.5.5 in /opt/spark/python (from pyspark-opendic==0.3.17) (3.5.5)\n",
      "Requirement already satisfied: requests>=2.32.3 in /usr/local/lib/python3.10/site-packages (from pyspark-opendic==0.3.17) (2.32.3)\n",
      "Requirement already satisfied: pandas>=2.2.3 in /usr/local/lib/python3.10/site-packages (from pyspark-opendic==0.3.17) (2.2.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas>=2.2.3->pyspark-opendic==0.3.17) (2025.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/site-packages (from pandas>=2.2.3->pyspark-opendic==0.3.17) (1.26.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/site-packages (from pandas>=2.2.3->pyspark-opendic==0.3.17) (2025.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/site-packages (from pandas>=2.2.3->pyspark-opendic==0.3.17) (2.9.0.post0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/site-packages (from pydantic>=2.10.6->pyspark-opendic==0.3.17) (0.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/site-packages (from pydantic>=2.10.6->pyspark-opendic==0.3.17) (4.12.2)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/site-packages (from pydantic>=2.10.6->pyspark-opendic==0.3.17) (2.27.2)\n",
      "Collecting py4j==0.10.9.7\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.3->pyspark-opendic==0.3.17) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.3->pyspark-opendic==0.3.17) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.3->pyspark-opendic==0.3.17) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.32.3->pyspark-opendic==0.3.17) (2.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=2.2.3->pyspark-opendic==0.3.17) (1.17.0)\n",
      "Installing collected packages: py4j, pyspark-opendic\n",
      "Successfully installed py4j-0.10.9.7 pyspark-opendic-0.3.17\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark-opendic==0.3.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8961c31e-19e6-4470-bbae-4611bf76f364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Running\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/01 10:10:32 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://6b60e2299c9b:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0xffff957431f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def read_secret(secret_name):\n",
    "    \"\"\" Get `secret_name` from docker-compose secret store\"\"\"\n",
    "    secret_path = f\"/run/secrets/{secret_name}\"\n",
    "    try:\n",
    "        with open(secret_path, \"r\") as f:\n",
    "            return f.read().strip()  # Remove any trailing newline\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Secret {secret_name} not found.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "## DEFINE SENSITIVE VARIABLES\n",
    "POLARIS_CATALOG_NAME = 'polaris'\n",
    "ENGINEER_CLIENT_ID = read_secret(\"engineer_client_id\")\n",
    "ENGINEER_CLIENT_SECRET = read_secret(\"engineer_client_secret\")\n",
    "ADLS_IO=\"org.apache.iceberg.azure.adlsv2.ADLSFileIO\"\n",
    "FILE_IO=\"org.apache.iceberg.io.ResolvingFileIO\"\n",
    "\n",
    "def create_session(client_id, client_secret, scope, fileio_impl):\n",
    "    spark = (SparkSession.builder\n",
    "        .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.0,software.amazon.awssdk:bundle:2.28.17,software.amazon.awssdk:url-connection-client:2.28.17\")\n",
    "        .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\")\n",
    "        .config(\"spark.sql.catalog.polaris\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "        .config(\"spark.sql.catalog.polaris.type\", \"rest\")\n",
    "        .config(\"spark.sql.catalog.polaris.warehouse\", POLARIS_CATALOG_NAME)\n",
    "        .config(\"spark.sql.catalog.polaris.uri\", 'http://polaris:8181/api/catalog')\n",
    "        .config(\"spark.sql.catalog.polaris.credential\", f\"{client_id}:{client_secret}\")\n",
    "        .config(\"spark.sql.catalog.polaris.scope\", 'PRINCIPAL_ROLE:ALL')\n",
    "        .config(\"spark.sql.defaultCatalog\", \"polaris\")\n",
    "        .config(\"spark.sql.catalogImplementation\", \"in-memory\")\n",
    "        .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\")\n",
    "        .config(\"spark.sql.catalog.polaris.token-refresh-enabled\", \"true\")\n",
    "        .config(\"spark.sql.catalog.polaris.header.X-Iceberg-Access-Delegation\", 'vended-credentials')\n",
    "        .config(\"spark.sql.catalog.polaris.io-impl\", fileio_impl)\n",
    "        .config(\"spark.history.fs.logDirectory\", \"/home/iceberg/spark-events\")).getOrCreate()\n",
    "        \n",
    "    print(\"Spark Running\")\n",
    "    return spark\n",
    "\n",
    "\n",
    "## Start Spark Session\n",
    "spark = create_session(client_id=ENGINEER_CLIENT_ID, client_secret=ENGINEER_CLIENT_SECRET, scope='PRINCIPAL_ROLE:ALL',fileio_impl=FILE_IO )\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192f7adc-e72e-4ae7-b436-4a3f1c7b8e57",
   "metadata": {},
   "source": [
    "## Setting up pyspark-opendic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27ccb2cb-8448-4111-b2af-bd3b19b92835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catalog initialized\n"
     ]
    }
   ],
   "source": [
    "from pyspark_opendic.catalog import OpenDicCatalog\n",
    "\n",
    "# Init polarisx catalog\n",
    "POLARIS_URI= \"http://polaris:8181/api\"\n",
    "catalog = OpenDicCatalog(spark, POLARIS_URI)\n",
    "print(\"Catalog initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30d98c61-b213-4bcd-b1ee-216dd8be9d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/01 10:10:36 WARN RESTSessionCatalog: Iceberg REST client is missing the OAuth2 server URI configuration and defaults to http://polaris:8181/api/catalog/v1/oauth/tokens. This automatic fallback will be removed in a future Iceberg release.It is recommended to configure the OAuth2 endpoint using the 'oauth2-server-uri' property to be prepared. This warning will disappear if the OAuth2 endpoint is explicitly configured. See https://github.com/apache/iceberg/issues/10537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|   SYSTEM|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "catalog.sql(\"Show namespaces\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68bd999-9f33-4c28-8568-dec8b3f6dbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "use SYSTEM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354fa57d-4e4b-40e3-b560-afdf7abaadc5",
   "metadata": {},
   "source": [
    "### Define the schema for a andfunc object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99808c3e-7fef-42f6-9a1e-4e44b798c69a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>udoType</th>\n",
       "      <th>properties</th>\n",
       "      <th>createdTimestamp</th>\n",
       "      <th>lastUpdatedTimestamp</th>\n",
       "      <th>version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>function_v2</td>\n",
       "      <td>{'return_type': 'STRING', 'created_time': 'STRING', 'entity_version': 'STRING', 'uname': 'STRING', 'def': 'STRING', 'signature': 'STRING', 'runtime': 'STRING', 'language': 'STRING', 'packages': 'STRING', 'args': 'STRING', 'last_updated_time': 'STRING', 'comment': 'STRING', 'client_version': 'STRING'}</td>\n",
       "      <td>1970-01-01T00:00Z</td>\n",
       "      <td>1970-01-01T00:00Z</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       udoType                                                                                                                                                                                                                                                                                                     properties   createdTimestamp lastUpdatedTimestamp version\n",
       "0  function_v2  {'return_type': 'STRING', 'created_time': 'STRING', 'entity_version': 'STRING', 'uname': 'STRING', 'def': 'STRING', 'signature': 'STRING', 'runtime': 'STRING', 'language': 'STRING', 'packages': 'STRING', 'args': 'STRING', 'last_updated_time': 'STRING', 'comment': 'STRING', 'client_version': 'STRING'}  1970-01-01T00:00Z    1970-01-01T00:00Z    None"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalog.sql(\n",
    "    \"\"\"\n",
    "    DEFINE OPEN function_v2\n",
    "    props {\n",
    "        \"args\": \"MAP\",\n",
    "        \"language\": \"STRING\",\n",
    "        \"def\": \"string\",\n",
    "        \"comment\": \"string\",\n",
    "        \"packages\": \"list\",\n",
    "        \"runtime\": \"string\",\n",
    "        \"client_version\": \"int\",\n",
    "        \"signature\": \"STRING\",\n",
    "        \"return_type\": \"STRING\"\n",
    "    }\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a117eb1-0929-4e30-9dea-c6980411f991",
   "metadata": {},
   "source": [
    "### Create a new andfunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59b83c05-2e49-449c-9fc3-f644acf682c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type-name</th>\n",
       "      <th>object-name</th>\n",
       "      <th>props</th>\n",
       "      <th>created-time-stamp</th>\n",
       "      <th>last-updated-time-stamp</th>\n",
       "      <th>entity-version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>function_v2</td>\n",
       "      <td>foo</td>\n",
       "      <td>{'args': {'arg1': 'int', 'arg2': 'int'}, 'return_type': 'int', 'def': 'def foo(arg1, arg2):\n",
       "\n",
       "        return arg1 + arg2', 'signature': 'foo(arg1 str, arg2 int)', 'runtime': '3.12', 'language': 'python', 'comment': 'test fun', 'packages': ['numpy', 'pandas'], 'client_version': 1}</td>\n",
       "      <td>2025-05-01T10:12:27.933377378Z</td>\n",
       "      <td>2025-05-01T10:12:27.933380669Z</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     type-name object-name                                                                                                                                                                                                                                                                                    props              created-time-stamp         last-updated-time-stamp  entity-version\n",
       "0  function_v2         foo  {'args': {'arg1': 'int', 'arg2': 'int'}, 'return_type': 'int', 'def': 'def foo(arg1, arg2):\n",
       "\n",
       "        return arg1 + arg2', 'signature': 'foo(arg1 str, arg2 int)', 'runtime': '3.12', 'language': 'python', 'comment': 'test fun', 'packages': ['numpy', 'pandas'], 'client_version': 1}  2025-05-01T10:12:27.933377378Z  2025-05-01T10:12:27.933380669Z               1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalog.sql(\n",
    " \"\"\"\n",
    " CREATE OPEN function_v2 foo\n",
    "    props {\n",
    "            \"args\": {\n",
    "                \"arg1\": \"int\", \n",
    "                \"arg2\": \"int\"\n",
    "                },\n",
    "            \"language\": \"python\",\n",
    "            \"def\": \"def foo(arg1, arg2):\\\\n\\\\n        return arg1 + arg2\",\n",
    "            \"packages\" : [\"numpy\", \"pandas\"],\n",
    "            \"comment\": \"test fun\",\n",
    "            \"runtime\": \"3.12\",\n",
    "            \"client_version\": 1,\n",
    "            \"return_type\": \"int\",\n",
    "            \"signature\": \"foo(arg1 str, arg2 int)\"\n",
    "        }\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc4ce21-1ded-4e92-8b3a-18f3d2264aa7",
   "metadata": {},
   "source": [
    "### Create Mapping to snowflake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3023dc59-10d0-42fe-8fa8-c9662c9b2078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>typeName</th>\n",
       "      <th>platformName</th>\n",
       "      <th>syntax</th>\n",
       "      <th>objectDumpMap</th>\n",
       "      <th>createdTimestamp</th>\n",
       "      <th>lastUpdatedTimestamp</th>\n",
       "      <th>version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>function_v2</td>\n",
       "      <td>snowflake</td>\n",
       "      <td>\"CREATE OR ALTER &lt;type&gt; &lt;name&gt;(&lt;args&gt;)\\n            RETURNS &lt;return_type&gt;\\n            LANGUAGE &lt;language&gt;\\n            RUNTIME = &lt;runtime&gt;\\n            HANDLER = '&lt;name&gt;'\\n            AS \\n            $$\\n            &lt;def&gt;\\n            $$\",</td>\n",
       "      <td>{'args': {'propType': 'map', 'format': '&lt;key&gt; &lt;value&gt;', 'delimiter': ', '}, 'packages': {'propType': 'list', 'format': ''&lt;item&gt;'', 'delimiter': ', '}}</td>\n",
       "      <td>2025-05-01T09:44:49.671430555Z</td>\n",
       "      <td>2025-05-01T09:44:49.671434180Z</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      typeName platformName                                                                                                                                                                                                                                             syntax                                                                                                                                           objectDumpMap                createdTimestamp            lastUpdatedTimestamp  version\n",
       "0  function_v2    snowflake  \"CREATE OR ALTER <type> <name>(<args>)\\n            RETURNS <return_type>\\n            LANGUAGE <language>\\n            RUNTIME = <runtime>\\n            HANDLER = '<name>'\\n            AS \\n            $$\\n            <def>\\n            $$\",  {'args': {'propType': 'map', 'format': '<key> <value>', 'delimiter': ', '}, 'packages': {'propType': 'list', 'format': ''<item>'', 'delimiter': ', '}}  2025-05-01T09:44:49.671430555Z  2025-05-01T09:44:49.671434180Z        1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalog.sql(\n",
    "    \"\"\"\n",
    "    ADD OPEN MAPPING function_v2 PLATFORM snowflake\n",
    "    SYNTAX {\n",
    "        \"CREATE OR ALTER <type> <name>(<args>)\n",
    "            RETURNS <return_type>\n",
    "            LANGUAGE <language>\n",
    "            RUNTIME = <runtime>\n",
    "            HANDLER = '<name>'\n",
    "            AS \n",
    "            $$\n",
    "            <def>\n",
    "            $$\",\n",
    "    }\n",
    "    PROPS {\n",
    "        \"args\": {\n",
    "                \"propType\": \"map\",\n",
    "                \"format\": \"<key> <value>\",\n",
    "                \"delimiter\": \", \"\n",
    "            },\n",
    "        \"packages\": {\"propType\": \"list\", \"format\": \"'<item>'\", \"delimiter\": \", \"}\n",
    "    }\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b019448-2b04-4063-a812-bb598122ef04",
   "metadata": {},
   "source": [
    "### List objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b13db8f-0e7c-4e0b-8b88-ba092fe91530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>udoType</th>\n",
       "      <th>properties</th>\n",
       "      <th>createdTimestamp</th>\n",
       "      <th>lastUpdatedTimestamp</th>\n",
       "      <th>version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>function</td>\n",
       "      <td>{'return_type': 'STRING', 'created_time': 'STRING', 'entity_version': 'STRING', 'uname': 'STRING', 'def': 'STRING', 'signature': 'STRING', 'runtime': 'STRING', 'language': 'STRING', 'packages': 'STRING', 'args': 'STRING', 'last_updated_time': 'STRING', 'comment': 'STRING', 'client_version': 'STRING'}</td>\n",
       "      <td>2025-05-01T09:40:03.047Z</td>\n",
       "      <td>2025-05-01T09:40:04.364Z</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>function_v2</td>\n",
       "      <td>{'return_type': 'STRING', 'created_time': 'STRING', 'entity_version': 'STRING', 'uname': 'STRING', 'def': 'STRING', 'signature': 'STRING', 'runtime': 'STRING', 'language': 'STRING', 'packages': 'STRING', 'args': 'STRING', 'last_updated_time': 'STRING', 'comment': 'STRING', 'client_version': 'STRING'}</td>\n",
       "      <td>2025-05-01T09:44:46.364Z</td>\n",
       "      <td>2025-05-01T09:44:46.364Z</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       udoType                                                                                                                                                                                                                                                                                                     properties          createdTimestamp      lastUpdatedTimestamp version\n",
       "0     function  {'return_type': 'STRING', 'created_time': 'STRING', 'entity_version': 'STRING', 'uname': 'STRING', 'def': 'STRING', 'signature': 'STRING', 'runtime': 'STRING', 'language': 'STRING', 'packages': 'STRING', 'args': 'STRING', 'last_updated_time': 'STRING', 'comment': 'STRING', 'client_version': 'STRING'}  2025-05-01T09:40:03.047Z  2025-05-01T09:40:04.364Z    None\n",
       "1  function_v2  {'return_type': 'STRING', 'created_time': 'STRING', 'entity_version': 'STRING', 'uname': 'STRING', 'def': 'STRING', 'signature': 'STRING', 'runtime': 'STRING', 'language': 'STRING', 'packages': 'STRING', 'args': 'STRING', 'last_updated_time': 'STRING', 'comment': 'STRING', 'client_version': 'STRING'}  2025-05-01T09:44:46.364Z  2025-05-01T09:44:46.364Z    None"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalog.sql(\n",
    "    \"\"\"\n",
    "    SHOW OPEN TYPES\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09d6a8bb-2989-4873-a8b4-e78416aea85f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>props</th>\n",
       "      <th>createdTimestamp</th>\n",
       "      <th>lastUpdatedTimestamp</th>\n",
       "      <th>entityVersion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>function_v2</td>\n",
       "      <td>foo</td>\n",
       "      <td>{'args': {'arg1': 'int', 'arg2': 'int'}, 'return_type': 'int', 'def': 'def foo(arg1, arg2):\n",
       "\n",
       "        return arg1 + arg2', 'signature': 'foo(arg1 str, arg2 int)', 'runtime': '3.12', 'language': 'python', 'comment': 'test fun', 'packages': ['numpy', 'pandas'], 'client_version': 1}</td>\n",
       "      <td>2025-05-01T09:44:46.027065511Z</td>\n",
       "      <td>2025-05-01T09:44:46.027068761Z</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          type name                                                                                                                                                                                                                                                                                    props                createdTimestamp            lastUpdatedTimestamp  entityVersion\n",
       "0  function_v2  foo  {'args': {'arg1': 'int', 'arg2': 'int'}, 'return_type': 'int', 'def': 'def foo(arg1, arg2):\n",
       "\n",
       "        return arg1 + arg2', 'signature': 'foo(arg1 str, arg2 int)', 'runtime': '3.12', 'language': 'python', 'comment': 'test fun', 'packages': ['numpy', 'pandas'], 'client_version': 1}  2025-05-01T09:44:46.027065511Z  2025-05-01T09:44:46.027068761Z              1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalog.sql(\n",
    "    \"\"\"\n",
    "    SHOW OPEN function_v2\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a08a0829-e1bc-40a4-b2e9-583369bc2785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>typeName</th>\n",
       "      <th>platformName</th>\n",
       "      <th>syntax</th>\n",
       "      <th>objectDumpMap</th>\n",
       "      <th>createdTimestamp</th>\n",
       "      <th>lastUpdatedTimestamp</th>\n",
       "      <th>version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>function_v2</td>\n",
       "      <td>snowflake</td>\n",
       "      <td>\"CREATE OR ALTER &lt;type&gt; &lt;name&gt;(&lt;args&gt;)\\n            RETURNS &lt;return_type&gt;\\n            LANGUAGE &lt;language&gt;\\n            RUNTIME = &lt;runtime&gt;\\n            HANDLER = '&lt;name&gt;'\\n            AS \\n            $$\\n            &lt;def&gt;\\n            $$\",</td>\n",
       "      <td>{'args': {'propType': 'map', 'format': '&lt;key&gt; &lt;value&gt;', 'delimiter': ', '}, 'packages': {'propType': 'list', 'format': ''&lt;item&gt;'', 'delimiter': ', '}}</td>\n",
       "      <td>2025-05-01T09:44:49.671430Z</td>\n",
       "      <td>2025-05-01T09:44:49.671434Z</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      typeName platformName                                                                                                                                                                                                                                             syntax                                                                                                                                           objectDumpMap             createdTimestamp         lastUpdatedTimestamp  version\n",
       "0  function_v2    snowflake  \"CREATE OR ALTER <type> <name>(<args>)\\n            RETURNS <return_type>\\n            LANGUAGE <language>\\n            RUNTIME = <runtime>\\n            HANDLER = '<name>'\\n            AS \\n            $$\\n            <def>\\n            $$\",  {'args': {'propType': 'map', 'format': '<key> <value>', 'delimiter': ', '}, 'packages': {'propType': 'list', 'format': ''<item>'', 'delimiter': ', '}}  2025-05-01T09:44:49.671430Z  2025-05-01T09:44:49.671434Z        1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show mapping for <object> to <platform>. Example: [Platform_mapping(function_v2 -> snowflake)]\n",
    "catalog.sql(\n",
    "    \"\"\"\n",
    "    SHOW OPEN MAPPING function_v2 PLATFORM snowflake\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "465f2f0d-4042-4304-92e7-9b08b1154757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>typeName</th>\n",
       "      <th>platformName</th>\n",
       "      <th>syntax</th>\n",
       "      <th>objectDumpMap</th>\n",
       "      <th>createdTimestamp</th>\n",
       "      <th>lastUpdatedTimestamp</th>\n",
       "      <th>version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>function_v2</td>\n",
       "      <td>snowflake</td>\n",
       "      <td>\"CREATE OR ALTER &lt;type&gt; &lt;name&gt;(&lt;args&gt;)\\n            RETURNS &lt;return_type&gt;\\n            LANGUAGE &lt;language&gt;\\n            RUNTIME = &lt;runtime&gt;\\n            HANDLER = '&lt;name&gt;'\\n            AS \\n            $$\\n            &lt;def&gt;\\n            $$\",</td>\n",
       "      <td>{'args': {'propType': 'map', 'format': '&lt;key&gt; &lt;value&gt;', 'delimiter': ', '}, 'packages': {'propType': 'list', 'format': ''&lt;item&gt;'', 'delimiter': ', '}}</td>\n",
       "      <td>2025-05-01T09:44:49.671430Z</td>\n",
       "      <td>2025-05-01T09:44:49.671434Z</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      typeName platformName                                                                                                                                                                                                                                             syntax                                                                                                                                           objectDumpMap             createdTimestamp         lastUpdatedTimestamp  version\n",
       "0  function_v2    snowflake  \"CREATE OR ALTER <type> <name>(<args>)\\n            RETURNS <return_type>\\n            LANGUAGE <language>\\n            RUNTIME = <runtime>\\n            HANDLER = '<name>'\\n            AS \\n            $$\\n            <def>\\n            $$\",  {'args': {'propType': 'map', 'format': '<key> <value>', 'delimiter': ', '}, 'packages': {'propType': 'list', 'format': ''<item>'', 'delimiter': ', '}}  2025-05-01T09:44:49.671430Z  2025-05-01T09:44:49.671434Z        1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show all mappings from <object>. Example: [snowflake,spark]\n",
    "catalog.sql(\n",
    "    \"\"\"\n",
    "    SHOW OPEN PLATFORMS FOR function_v2\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d38ccc6c-fac8-4d6c-a9d6-a556d5a91037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>platformName</th>\n",
       "      <th>platformMappings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>snowflake</td>\n",
       "      <td>[{'typeName': 'function_v2', 'platformName': 'snowflake', 'syntax': '\"CREATE OR ALTER &lt;type&gt; &lt;name&gt;(&lt;args&gt;)\n",
       "            RETURNS &lt;return_type&gt;\n",
       "            LANGUAGE &lt;language&gt;\n",
       "            RUNTIME = &lt;runtime&gt;\n",
       "            HANDLER = '&lt;name&gt;'\n",
       "            AS \n",
       "            $$\n",
       "            &lt;def&gt;\n",
       "            $$\",', 'objectDumpMap': {'args': {'propType': 'map', 'format': '&lt;key&gt; &lt;value&gt;', 'delimiter': ', '}, 'packages': {'propType': 'list', 'format': \"'&lt;item&gt;'\", 'delimiter': ', '}}, 'createdTimestamp': '2025-05-01T09:44:49.671430Z', 'lastUpdatedTimestamp': '2025-05-01T09:44:49.671434Z', 'version': 1}, {'typeName': 'function', 'platformName': 'snowflake', 'syntax': 'CREATE OR ALTER &lt;type&gt; &lt;name&gt;(&lt;args&gt;)\n",
       "    RETURNS &lt;return_type&gt;\n",
       "    LANGUAGE &lt;language&gt;\n",
       "    PACKAGES = (&lt;packages&gt;)\n",
       "    RUNTIME = &lt;runtime&gt;\n",
       "    HANDLER = '&lt;name&gt;'\n",
       "    AS $$\n",
       "    &lt;def&gt;\n",
       "    $$\n",
       "', 'objectDumpMap': {'args': {'propType': 'map', 'format': '&lt;key&gt; &lt;value&gt;', 'delimiter': ', '}, 'packages': {'propType': 'list', 'format': \"'&lt;item&gt;'\", 'delimiter': ', '}}, 'createdTimestamp': '2025-05-01T09:40:03.342585Z', 'lastUpdatedTimestamp': '2025-05-01T09:40:03.342587Z', 'version': 1}]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  platformName                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        platformMappings\n",
       "0    snowflake  [{'typeName': 'function_v2', 'platformName': 'snowflake', 'syntax': '\"CREATE OR ALTER <type> <name>(<args>)\n",
       "            RETURNS <return_type>\n",
       "            LANGUAGE <language>\n",
       "            RUNTIME = <runtime>\n",
       "            HANDLER = '<name>'\n",
       "            AS \n",
       "            $$\n",
       "            <def>\n",
       "            $$\",', 'objectDumpMap': {'args': {'propType': 'map', 'format': '<key> <value>', 'delimiter': ', '}, 'packages': {'propType': 'list', 'format': \"'<item>'\", 'delimiter': ', '}}, 'createdTimestamp': '2025-05-01T09:44:49.671430Z', 'lastUpdatedTimestamp': '2025-05-01T09:44:49.671434Z', 'version': 1}, {'typeName': 'function', 'platformName': 'snowflake', 'syntax': 'CREATE OR ALTER <type> <name>(<args>)\n",
       "    RETURNS <return_type>\n",
       "    LANGUAGE <language>\n",
       "    PACKAGES = (<packages>)\n",
       "    RUNTIME = <runtime>\n",
       "    HANDLER = '<name>'\n",
       "    AS $$\n",
       "    <def>\n",
       "    $$\n",
       "', 'objectDumpMap': {'args': {'propType': 'map', 'format': '<key> <value>', 'delimiter': ', '}, 'packages': {'propType': 'list', 'format': \"'<item>'\", 'delimiter': ', '}}, 'createdTimestamp': '2025-05-01T09:40:03.342585Z', 'lastUpdatedTimestamp': '2025-05-01T09:40:03.342587Z', 'version': 1}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalog.sql(\n",
    "    \"\"\"\n",
    "    SHOW OPEN PLATFORMS\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e9d02df-c4f4-46f4-b707-99d3d2869d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>typeName</th>\n",
       "      <th>platformName</th>\n",
       "      <th>syntax</th>\n",
       "      <th>objectDumpMap</th>\n",
       "      <th>createdTimestamp</th>\n",
       "      <th>lastUpdatedTimestamp</th>\n",
       "      <th>version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>function</td>\n",
       "      <td>snowflake</td>\n",
       "      <td>CREATE OR ALTER &lt;type&gt; &lt;name&gt;(&lt;args&gt;)\\n    RETURNS &lt;return_type&gt;\\n    LANGUAGE &lt;language&gt;\\n    PACKAGES = (&lt;packages&gt;)\\n    RUNTIME = &lt;runtime&gt;\\n    HANDLER = '&lt;name&gt;'\\n    AS $$\\n    &lt;def&gt;\\n    $$\\n</td>\n",
       "      <td>{'args': {'propType': 'map', 'format': '&lt;key&gt; &lt;value&gt;', 'delimiter': ', '}, 'packages': {'propType': 'list', 'format': ''&lt;item&gt;'', 'delimiter': ', '}}</td>\n",
       "      <td>2025-05-01T09:40:03.342585Z</td>\n",
       "      <td>2025-05-01T09:40:03.342587Z</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>function_v2</td>\n",
       "      <td>snowflake</td>\n",
       "      <td>\"CREATE OR ALTER &lt;type&gt; &lt;name&gt;(&lt;args&gt;)\\n            RETURNS &lt;return_type&gt;\\n            LANGUAGE &lt;language&gt;\\n            RUNTIME = &lt;runtime&gt;\\n            HANDLER = '&lt;name&gt;'\\n            AS \\n            $$\\n            &lt;def&gt;\\n            $$\",</td>\n",
       "      <td>{'args': {'propType': 'map', 'format': '&lt;key&gt; &lt;value&gt;', 'delimiter': ', '}, 'packages': {'propType': 'list', 'format': ''&lt;item&gt;'', 'delimiter': ', '}}</td>\n",
       "      <td>2025-05-01T09:44:49.671430Z</td>\n",
       "      <td>2025-05-01T09:44:49.671434Z</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      typeName platformName                                                                                                                                                                                                                                             syntax                                                                                                                                           objectDumpMap             createdTimestamp         lastUpdatedTimestamp  version\n",
       "0     function    snowflake                                            CREATE OR ALTER <type> <name>(<args>)\\n    RETURNS <return_type>\\n    LANGUAGE <language>\\n    PACKAGES = (<packages>)\\n    RUNTIME = <runtime>\\n    HANDLER = '<name>'\\n    AS $$\\n    <def>\\n    $$\\n  {'args': {'propType': 'map', 'format': '<key> <value>', 'delimiter': ', '}, 'packages': {'propType': 'list', 'format': ''<item>'', 'delimiter': ', '}}  2025-05-01T09:40:03.342585Z  2025-05-01T09:40:03.342587Z        1\n",
       "1  function_v2    snowflake  \"CREATE OR ALTER <type> <name>(<args>)\\n            RETURNS <return_type>\\n            LANGUAGE <language>\\n            RUNTIME = <runtime>\\n            HANDLER = '<name>'\\n            AS \\n            $$\\n            <def>\\n            $$\",  {'args': {'propType': 'map', 'format': '<key> <value>', 'delimiter': ', '}, 'packages': {'propType': 'list', 'format': ''<item>'', 'delimiter': ', '}}  2025-05-01T09:44:49.671430Z  2025-05-01T09:44:49.671434Z        1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalog.sql(\n",
    "    \"\"\"\n",
    "    SHOW OPEN MAPPINGS FOR snowflake\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac2991f5-6099-4bc0-9e64-dda501c6104f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing 1 SQL statements...\n",
      "\n",
      "Formatted SQL:\n",
      "\"CREATE OR ALTER function_v2 foo(arg1 int, arg2 int)\n",
      "            RETURNS int\n",
      "            LANGUAGE python\n",
      "            RUNTIME = 3.12\n",
      "            HANDLER = 'foo'\n",
      "            AS \n",
      "            $$\n",
      "            def foo(arg1, arg2):\n",
      "\n",
      "        return arg1 + arg2\n",
      "            $$\",\n",
      "Executing SQL (multilined SQL): \n",
      "\"\"\"\n",
      "\"CREATE OR ALTER function_v2 foo(arg1 int, arg2 int)\n",
      "            RETURNS int\n",
      "            LANGUAGE python\n",
      "            RUNTIME = 3.12\n",
      "            HANDLER = 'foo'\n",
      "            AS \n",
      "            $$\n",
      "            def foo(arg1, arg2):\n",
      "\n",
      "        return arg1 + arg2\n",
      "            $$\",\n",
      "\"\"\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "    \"executions\": [\n",
       "        {\n",
       "            \"sql\": \"\\\"CREATE OR ALTER function_v2 foo(arg1 int, arg2 int)\\n            RETURNS int\\n            LANGUAGE python\\n            RUNTIME = 3.12\\n            HANDLER = 'foo'\\n            AS \\n            $$\\n            def foo(arg1, arg2):\\n\\n        return arg1 + arg2\\n            $$\\\",\",\n",
       "            \"status\": \"failed\",\n",
       "            \"error\": \"\\n[PARSE_SYNTAX_ERROR] Syntax error at or near '\\\"\\\"'.(line 1, pos 0)\\n\\n== SQL ==\\n\\\"\\\"\\\"\\n^^^\\n\\\"CREATE OR ALTER function_v2 foo(arg1 int, arg2 int)\\n            RETURNS int\\n            LANGUAGE python\\n            RUNTIME = 3.12\\n            HANDLER = 'foo'\\n            AS \\n            $$\\n            def foo(arg1, arg2):\\n\\n        return arg1 + arg2\\n            $$\\\",\\n\\\"\\\"\\\"\\n\"\n",
       "        }\n",
       "    ]\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "{\n",
       "    \"executions\": [\n",
       "        {\n",
       "            \"sql\": \"\\\"CREATE OR ALTER function_v2 foo(arg1 int, arg2 int)\\n            RETURNS int\\n            LANGUAGE python\\n            RUNTIME = 3.12\\n            HANDLER = 'foo'\\n            AS \\n            $$\\n            def foo(arg1, arg2):\\n\\n        return arg1 + arg2\\n            $$\\\",\",\n",
       "            \"status\": \"failed\",\n",
       "            \"error\": \"\\n[PARSE_SYNTAX_ERROR] Syntax error at or near '\\\"\\\"'.(line 1, pos 0)\\n\\n== SQL ==\\n\\\"\\\"\\\"\\n^^^\\n\\\"CREATE OR ALTER function_v2 foo(arg1 int, arg2 int)\\n            RETURNS int\\n            LANGUAGE python\\n            RUNTIME = 3.12\\n            HANDLER = 'foo'\\n            AS \\n            $$\\n            def foo(arg1, arg2):\\n\\n        return arg1 + arg2\\n            $$\\\",\\n\\\"\\\"\\\"\\n\"\n",
       "        }\n",
       "    ]\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalog.sql(\n",
    "    \"\"\"\n",
    "    SYNC OPEN function_v2 for snowflake\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e18be6e2-30ca-44ae-af80-da349d61916d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "    \"error\": \"HTTP Error\",\n",
       "    \"exception message\": \"500 Server Error: Internal Server Error for url: http://polaris:8181/api/opendic/v1/platforms/snowflake/pull\",\n",
       "    \"Catalog Response\": {\n",
       "        \"error\": {\n",
       "            \"message\": null,\n",
       "            \"type\": \"NullPointerException\",\n",
       "            \"code\": 500\n",
       "        }\n",
       "    }\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "{\n",
       "    \"error\": \"HTTP Error\",\n",
       "    \"exception message\": \"500 Server Error: Internal Server Error for url: http://polaris:8181/api/opendic/v1/platforms/snowflake/pull\",\n",
       "    \"Catalog Response\": {\n",
       "        \"error\": {\n",
       "            \"message\": null,\n",
       "            \"type\": \"NullPointerException\",\n",
       "            \"code\": 500\n",
       "        }\n",
       "    }\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalog.sql(\n",
    "    \"\"\"\n",
    "    SYNC OPEN OBJECTS for snowflake\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a13475c-abcc-4bed-96d5-0d9044aee319",
   "metadata": {},
   "source": [
    "### Drop andfunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62ba6692-c3a3-45e8-b1a6-a7d33697a990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "    \"error\": \"HTTP Error\",\n",
       "    \"exception message\": \"400 Client Error: Bad Request for url: http://polaris:8181/api/opendic/v1/objects/function_v2\",\n",
       "    \"Catalog Response\": {\n",
       "        \"error\": {\n",
       "            \"message\": \"An exception occurred while creating a query in EntityManager: \\nException Description: Problem compiling [SELECT m from ModelPolicyMappingRecord m  where m.targetCatalogId=:targetCatalogId and m.targetId=:targetId]. \\n[14, 38] The abstract schema type 'ModelPolicyMappingRecord' is unknown.\\n[47, 64] The state field path 'm.targetCatalogId' cannot be resolved to a valid type.\\n[88, 98] The state field path 'm.targetId' cannot be resolved to a valid type.\",\n",
       "            \"type\": \"IllegalArgumentException\",\n",
       "            \"code\": 400\n",
       "        }\n",
       "    }\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "{\n",
       "    \"error\": \"HTTP Error\",\n",
       "    \"exception message\": \"400 Client Error: Bad Request for url: http://polaris:8181/api/opendic/v1/objects/function_v2\",\n",
       "    \"Catalog Response\": {\n",
       "        \"error\": {\n",
       "            \"message\": \"An exception occurred while creating a query in EntityManager: \\nException Description: Problem compiling [SELECT m from ModelPolicyMappingRecord m  where m.targetCatalogId=:targetCatalogId and m.targetId=:targetId]. \\n[14, 38] The abstract schema type 'ModelPolicyMappingRecord' is unknown.\\n[47, 64] The state field path 'm.targetCatalogId' cannot be resolved to a valid type.\\n[88, 98] The state field path 'm.targetId' cannot be resolved to a valid type.\",\n",
       "            \"type\": \"IllegalArgumentException\",\n",
       "            \"code\": 400\n",
       "        }\n",
       "    }\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalog.sql(\n",
    "    \"\"\"\n",
    "    DROP OPEN function_v2\n",
    "    \"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "159960df-32ec-4d43-a16a-86e32780e4a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```json\n",
       "{\n",
       "    \"error\": \"HTTP Error\",\n",
       "    \"exception message\": \"404 Client Error: Not Found for url: http://polaris:8181/api/opendic/v1/platforms/snowflake\",\n",
       "    \"Catalog Response\": {\n",
       "        \"No mappings for platform snowflake found\": \"snowflake\"\n",
       "    }\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "{\n",
       "    \"error\": \"HTTP Error\",\n",
       "    \"exception message\": \"404 Client Error: Not Found for url: http://polaris:8181/api/opendic/v1/platforms/snowflake\",\n",
       "    \"Catalog Response\": {\n",
       "        \"No mappings for platform snowflake found\": \"snowflake\"\n",
       "    }\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catalog.sql(\n",
    "    \"\"\"\n",
    "    DROP OPEN MAPPINGS for snowflake\n",
    "    \"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f67035-aa7b-4eab-8ba0-7e7132c68106",
   "metadata": {},
   "source": [
    "### Visualize opendic tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66730eb2-aa42-42b5-8f15-f6fb6c3908d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>namespace</th>\n",
       "            <th>tableName</th>\n",
       "            <th>isTemporary</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>SYSTEM</td>\n",
       "            <td>function</td>\n",
       "            <td>False</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>SYSTEM</td>\n",
       "            <td>function_v2</td>\n",
       "            <td>False</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+-----------+-------------+-------------+\n",
       "| namespace |   tableName | isTemporary |\n",
       "+-----------+-------------+-------------+\n",
       "|    SYSTEM |    function |       False |\n",
       "|    SYSTEM | function_v2 |       False |\n",
       "+-----------+-------------+-------------+"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "show tables in SYSTEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ef1708a-3517-4c09-aaad-f51d08929cac",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o146.collectToPython.\n: org.apache.iceberg.exceptions.NotFoundException: Failed to open input stream for file: file:/data/SYSTEM/function_v2/metadata/snap-397138463769984362-1-da961996-5028-4c2a-808b-d60355c51f14.avro\n\tat org.apache.iceberg.hadoop.HadoopInputFile.newStream(HadoopInputFile.java:185)\n\tat org.apache.iceberg.avro.AvroIterable.newFileReader(AvroIterable.java:102)\n\tat org.apache.iceberg.avro.AvroIterable.iterator(AvroIterable.java:77)\n\tat org.apache.iceberg.avro.AvroIterable.iterator(AvroIterable.java:37)\n\tat org.apache.iceberg.relocated.com.google.common.collect.Iterables.addAll(Iterables.java:333)\n\tat org.apache.iceberg.relocated.com.google.common.collect.Lists.newLinkedList(Lists.java:260)\n\tat org.apache.iceberg.ManifestLists.read(ManifestLists.java:45)\n\tat org.apache.iceberg.BaseSnapshot.cacheManifests(BaseSnapshot.java:165)\n\tat org.apache.iceberg.BaseSnapshot.deleteManifests(BaseSnapshot.java:199)\n\tat org.apache.iceberg.BaseDistributedDataScan.findMatchingDeleteManifests(BaseDistributedDataScan.java:208)\n\tat org.apache.iceberg.BaseDistributedDataScan.doPlanFiles(BaseDistributedDataScan.java:149)\n\tat org.apache.iceberg.SnapshotScan.planFiles(SnapshotScan.java:139)\n\tat org.apache.iceberg.spark.source.SparkPartitioningAwareScan.tasks(SparkPartitioningAwareScan.java:174)\n\tat org.apache.iceberg.spark.source.SparkPartitioningAwareScan.taskGroups(SparkPartitioningAwareScan.java:202)\n\tat org.apache.iceberg.spark.source.SparkPartitioningAwareScan.outputPartitioning(SparkPartitioningAwareScan.java:104)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ScanPartitioningAndOrdering$$anonfun$partitioning$1.applyOrElse(V2ScanPartitioningAndOrdering.scala:44)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ScanPartitioningAndOrdering$$anonfun$partitioning$1.applyOrElse(V2ScanPartitioningAndOrdering.scala:42)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.LocalLimit.mapChildren(basicLogicalOperators.scala:1608)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.GlobalLimit.mapChildren(basicLogicalOperators.scala:1587)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ScanPartitioningAndOrdering$.partitioning(V2ScanPartitioningAndOrdering.scala:42)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ScanPartitioningAndOrdering$.$anonfun$apply$1(V2ScanPartitioningAndOrdering.scala:35)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ScanPartitioningAndOrdering$.$anonfun$apply$3(V2ScanPartitioningAndOrdering.scala:38)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ScanPartitioningAndOrdering$.apply(V2ScanPartitioningAndOrdering.scala:37)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ScanPartitioningAndOrdering$.apply(V2ScanPartitioningAndOrdering.scala:33)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)\n\tat org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)\n\tat org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4146)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.io.FileNotFoundException: File file:/data/SYSTEM/function_v2/metadata/snap-397138463769984362-1-da961996-5028-4c2a-808b-d60355c51f14.avro does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)\n\tat org.apache.iceberg.hadoop.HadoopInputFile.newStream(HadoopInputFile.java:183)\n\t... 98 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msql\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mselect * from SYSTEM.function_v2\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2543\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2541\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2542\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2543\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2545\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2547\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2548\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/.ipython/profile_default/startup/00-prettytables.py:81\u001b[0m, in \u001b[0;36msql\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _to_table(df, num_rows\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mlimit)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_to_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.ipython/profile_default/startup/00-prettytables.py:47\u001b[0m, in \u001b[0;36m_to_table\u001b[0;34m(df, num_rows)\u001b[0m\n\u001b[1;32m     45\u001b[0m t\u001b[38;5;241m.\u001b[39mfield_names \u001b[38;5;241m=\u001b[39m cols\n\u001b[1;32m     46\u001b[0m t\u001b[38;5;241m.\u001b[39malign \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_rows\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     48\u001b[0m     d \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39masDict()\n\u001b[1;32m     49\u001b[0m     t\u001b[38;5;241m.\u001b[39madd_row([ d[col] \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols ])\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:1263\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1243\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m \n\u001b[1;32m   1245\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1263\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o146.collectToPython.\n: org.apache.iceberg.exceptions.NotFoundException: Failed to open input stream for file: file:/data/SYSTEM/function_v2/metadata/snap-397138463769984362-1-da961996-5028-4c2a-808b-d60355c51f14.avro\n\tat org.apache.iceberg.hadoop.HadoopInputFile.newStream(HadoopInputFile.java:185)\n\tat org.apache.iceberg.avro.AvroIterable.newFileReader(AvroIterable.java:102)\n\tat org.apache.iceberg.avro.AvroIterable.iterator(AvroIterable.java:77)\n\tat org.apache.iceberg.avro.AvroIterable.iterator(AvroIterable.java:37)\n\tat org.apache.iceberg.relocated.com.google.common.collect.Iterables.addAll(Iterables.java:333)\n\tat org.apache.iceberg.relocated.com.google.common.collect.Lists.newLinkedList(Lists.java:260)\n\tat org.apache.iceberg.ManifestLists.read(ManifestLists.java:45)\n\tat org.apache.iceberg.BaseSnapshot.cacheManifests(BaseSnapshot.java:165)\n\tat org.apache.iceberg.BaseSnapshot.deleteManifests(BaseSnapshot.java:199)\n\tat org.apache.iceberg.BaseDistributedDataScan.findMatchingDeleteManifests(BaseDistributedDataScan.java:208)\n\tat org.apache.iceberg.BaseDistributedDataScan.doPlanFiles(BaseDistributedDataScan.java:149)\n\tat org.apache.iceberg.SnapshotScan.planFiles(SnapshotScan.java:139)\n\tat org.apache.iceberg.spark.source.SparkPartitioningAwareScan.tasks(SparkPartitioningAwareScan.java:174)\n\tat org.apache.iceberg.spark.source.SparkPartitioningAwareScan.taskGroups(SparkPartitioningAwareScan.java:202)\n\tat org.apache.iceberg.spark.source.SparkPartitioningAwareScan.outputPartitioning(SparkPartitioningAwareScan.java:104)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ScanPartitioningAndOrdering$$anonfun$partitioning$1.applyOrElse(V2ScanPartitioningAndOrdering.scala:44)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ScanPartitioningAndOrdering$$anonfun$partitioning$1.applyOrElse(V2ScanPartitioningAndOrdering.scala:42)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.LocalLimit.mapChildren(basicLogicalOperators.scala:1608)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1216)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1215)\n\tat org.apache.spark.sql.catalyst.plans.logical.GlobalLimit.mapChildren(basicLogicalOperators.scala:1587)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:466)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ScanPartitioningAndOrdering$.partitioning(V2ScanPartitioningAndOrdering.scala:42)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ScanPartitioningAndOrdering$.$anonfun$apply$1(V2ScanPartitioningAndOrdering.scala:35)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ScanPartitioningAndOrdering$.$anonfun$apply$3(V2ScanPartitioningAndOrdering.scala:38)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ScanPartitioningAndOrdering$.apply(V2ScanPartitioningAndOrdering.scala:37)\n\tat org.apache.spark.sql.execution.datasources.v2.V2ScanPartitioningAndOrdering$.apply(V2ScanPartitioningAndOrdering.scala:33)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:222)\n\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:219)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:211)\n\tat scala.collection.immutable.List.foreach(List.scala:431)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:211)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:152)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:219)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:219)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:218)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:148)\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:144)\n\tat org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:162)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:182)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:179)\n\tat org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:238)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:284)\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:252)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:117)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4146)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: java.io.FileNotFoundException: File file:/data/SYSTEM/function_v2/metadata/snap-397138463769984362-1-da961996-5028-4c2a-808b-d60355c51f14.avro does not exist\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:160)\n\tat org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:372)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:976)\n\tat org.apache.iceberg.hadoop.HadoopInputFile.newStream(HadoopInputFile.java:183)\n\t... 98 more\n"
     ]
    }
   ],
   "source": [
    "%%sql\n",
    "select * from SYSTEM.function_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dbb219f-304b-47c3-91b1-6bf220580183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>col_name</th>\n",
       "            <th>data_type</th>\n",
       "            <th>comment</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>uname</td>\n",
       "            <td>string</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>args</td>\n",
       "            <td>map&lt;string,string&gt;</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>return_type</td>\n",
       "            <td>string</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>def</td>\n",
       "            <td>string</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>signature</td>\n",
       "            <td>string</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>runtime</td>\n",
       "            <td>string</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>language</td>\n",
       "            <td>string</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>comment</td>\n",
       "            <td>string</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>packages</td>\n",
       "            <td>array&lt;string&gt;</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>client_version</td>\n",
       "            <td>int</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>created_time</td>\n",
       "            <td>string</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>last_updated_time</td>\n",
       "            <td>string</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>entity_version</td>\n",
       "            <td>int</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td></td>\n",
       "            <td></td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td># Metadata Columns</td>\n",
       "            <td></td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>_spec_id</td>\n",
       "            <td>int</td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>_partition</td>\n",
       "            <td>struct&lt;&gt;</td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>_file</td>\n",
       "            <td>string</td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>_pos</td>\n",
       "            <td>bigint</td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>_deleted</td>\n",
       "            <td>boolean</td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td></td>\n",
       "            <td></td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td># Detailed Table Information</td>\n",
       "            <td></td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Name</td>\n",
       "            <td>polaris.SYSTEM.function</td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Type</td>\n",
       "            <td>MANAGED</td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Location</td>\n",
       "            <td>file:///data/SYSTEM/function</td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Provider</td>\n",
       "            <td>iceberg</td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Table Properties</td>\n",
       "            <td>[created-at=2025-05-01T10:06:36.360947715Z,current-snapshot-id=3475100123274412795,format=iceberg/parquet,format-version=2,write.parquet.compression-codec=zstd]</td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+\n",
       "|                     col_name |                                                                                                                                                        data_type | comment |\n",
       "+------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+\n",
       "|                        uname |                                                                                                                                                           string |    None |\n",
       "|                         args |                                                                                                                                               map<string,string> |    None |\n",
       "|                  return_type |                                                                                                                                                           string |    None |\n",
       "|                          def |                                                                                                                                                           string |    None |\n",
       "|                    signature |                                                                                                                                                           string |    None |\n",
       "|                      runtime |                                                                                                                                                           string |    None |\n",
       "|                     language |                                                                                                                                                           string |    None |\n",
       "|                      comment |                                                                                                                                                           string |    None |\n",
       "|                     packages |                                                                                                                                                    array<string> |    None |\n",
       "|               client_version |                                                                                                                                                              int |    None |\n",
       "|                 created_time |                                                                                                                                                           string |    None |\n",
       "|            last_updated_time |                                                                                                                                                           string |    None |\n",
       "|               entity_version |                                                                                                                                                              int |    None |\n",
       "|                              |                                                                                                                                                                  |         |\n",
       "|           # Metadata Columns |                                                                                                                                                                  |         |\n",
       "|                     _spec_id |                                                                                                                                                              int |         |\n",
       "|                   _partition |                                                                                                                                                         struct<> |         |\n",
       "|                        _file |                                                                                                                                                           string |         |\n",
       "|                         _pos |                                                                                                                                                           bigint |         |\n",
       "|                     _deleted |                                                                                                                                                          boolean |         |\n",
       "|                              |                                                                                                                                                                  |         |\n",
       "| # Detailed Table Information |                                                                                                                                                                  |         |\n",
       "|                         Name |                                                                                                                                          polaris.SYSTEM.function |         |\n",
       "|                         Type |                                                                                                                                                          MANAGED |         |\n",
       "|                     Location |                                                                                                                                     file:///data/SYSTEM/function |         |\n",
       "|                     Provider |                                                                                                                                                          iceberg |         |\n",
       "|             Table Properties | [created-at=2025-05-01T10:06:36.360947715Z,current-snapshot-id=3475100123274412795,format=iceberg/parquet,format-version=2,write.parquet.compression-codec=zstd] |         |\n",
       "+------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "DESCRIBE EXTENDED SYSTEM.function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9a9f41",
   "metadata": {},
   "source": [
    "## Load One Month of NYC Taxi/Limousine Trip Data\n",
    "\n",
    "For this notebook, we will use the New York City Taxi and Limousine Commision Trip Record Data that's available on the AWS Open Data Registry. This contains data of trips taken by taxis and for-hire vehicles in New York City. We'll save this into an iceberg table called `taxis`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747bee98",
   "metadata": {},
   "source": [
    "To be able to rerun the notebook several times, let's drop the table if it exists to start fresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "930682ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "++\n",
       "||\n",
       "++\n",
       "++"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE DATABASE IF NOT EXISTS nyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f918310a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "An exception occurred while creating a query in EntityManager: \nException Description: Problem compiling [SELECT m from ModelPolicyMappingRecord m  where m.targetCatalogId=:targetCatalogId and m.targetId=:targetId]. \n[14, 38] The abstract schema type 'ModelPolicyMappingRecord' is unknown.\n[47, 64] The state field path 'm.targetCatalogId' cannot be resolved to a valid type.\n[88, 98] The state field path 'm.targetId' cannot be resolved to a valid type.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msql\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mDROP TABLE IF EXISTS nyc.taxis\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:2543\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2541\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2542\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2543\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2545\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2547\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2548\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/.ipython/profile_default/startup/00-prettytables.py:81\u001b[0m, in \u001b[0;36msql\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _to_table(df, num_rows\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mlimit)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _to_table(\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: An exception occurred while creating a query in EntityManager: \nException Description: Problem compiling [SELECT m from ModelPolicyMappingRecord m  where m.targetCatalogId=:targetCatalogId and m.targetId=:targetId]. \n[14, 38] The abstract schema type 'ModelPolicyMappingRecord' is unknown.\n[47, 64] The state field path 'm.targetCatalogId' cannot be resolved to a valid type.\n[88, 98] The state field path 'm.targetId' cannot be resolved to a valid type."
     ]
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "DROP TABLE IF EXISTS nyc.taxis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c37ca92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"/home/iceberg/data/yellow_tripdata_2021-04.parquet\")\n",
    "df.write.saveAsTable(\"nyc.taxis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9fddb808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>col_name</th>\n",
       "            <th>data_type</th>\n",
       "            <th>comment</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>VendorID</td>\n",
       "            <td>bigint</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>tpep_pickup_datetime</td>\n",
       "            <td>timestamp_ntz</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>tpep_dropoff_datetime</td>\n",
       "            <td>timestamp_ntz</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>passenger_count</td>\n",
       "            <td>double</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>trip_distance</td>\n",
       "            <td>double</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>RatecodeID</td>\n",
       "            <td>double</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>store_and_fwd_flag</td>\n",
       "            <td>string</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>PULocationID</td>\n",
       "            <td>bigint</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>DOLocationID</td>\n",
       "            <td>bigint</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>payment_type</td>\n",
       "            <td>bigint</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>fare_amount</td>\n",
       "            <td>double</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>extra</td>\n",
       "            <td>double</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>mta_tax</td>\n",
       "            <td>double</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>tip_amount</td>\n",
       "            <td>double</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>tolls_amount</td>\n",
       "            <td>double</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>improvement_surcharge</td>\n",
       "            <td>double</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>total_amount</td>\n",
       "            <td>double</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>congestion_surcharge</td>\n",
       "            <td>double</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>airport_fee</td>\n",
       "            <td>double</td>\n",
       "            <td>None</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td></td>\n",
       "            <td></td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td># Metadata Columns</td>\n",
       "            <td></td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>_spec_id</td>\n",
       "            <td>int</td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>_partition</td>\n",
       "            <td>struct&lt;&gt;</td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>_file</td>\n",
       "            <td>string</td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>_pos</td>\n",
       "            <td>bigint</td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>_deleted</td>\n",
       "            <td>boolean</td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td></td>\n",
       "            <td></td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td># Detailed Table Information</td>\n",
       "            <td></td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Name</td>\n",
       "            <td>polaris.nyc.taxis</td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Type</td>\n",
       "            <td>MANAGED</td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Location</td>\n",
       "            <td>file:///data/nyc/taxis</td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Provider</td>\n",
       "            <td>iceberg</td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Owner</td>\n",
       "            <td>root</td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>Table Properties</td>\n",
       "            <td>[created-at=2025-05-01T10:11:24.523358293Z,current-snapshot-id=1227733769855991745,format=iceberg/parquet,format-version=2,write.format.default=parquet,write.parquet.compression-codec=zstd]</td>\n",
       "            <td></td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+\n",
       "|                     col_name |                                                                                                                                                                                     data_type | comment |\n",
       "+------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+\n",
       "|                     VendorID |                                                                                                                                                                                        bigint |    None |\n",
       "|         tpep_pickup_datetime |                                                                                                                                                                                 timestamp_ntz |    None |\n",
       "|        tpep_dropoff_datetime |                                                                                                                                                                                 timestamp_ntz |    None |\n",
       "|              passenger_count |                                                                                                                                                                                        double |    None |\n",
       "|                trip_distance |                                                                                                                                                                                        double |    None |\n",
       "|                   RatecodeID |                                                                                                                                                                                        double |    None |\n",
       "|           store_and_fwd_flag |                                                                                                                                                                                        string |    None |\n",
       "|                 PULocationID |                                                                                                                                                                                        bigint |    None |\n",
       "|                 DOLocationID |                                                                                                                                                                                        bigint |    None |\n",
       "|                 payment_type |                                                                                                                                                                                        bigint |    None |\n",
       "|                  fare_amount |                                                                                                                                                                                        double |    None |\n",
       "|                        extra |                                                                                                                                                                                        double |    None |\n",
       "|                      mta_tax |                                                                                                                                                                                        double |    None |\n",
       "|                   tip_amount |                                                                                                                                                                                        double |    None |\n",
       "|                 tolls_amount |                                                                                                                                                                                        double |    None |\n",
       "|        improvement_surcharge |                                                                                                                                                                                        double |    None |\n",
       "|                 total_amount |                                                                                                                                                                                        double |    None |\n",
       "|         congestion_surcharge |                                                                                                                                                                                        double |    None |\n",
       "|                  airport_fee |                                                                                                                                                                                        double |    None |\n",
       "|                              |                                                                                                                                                                                               |         |\n",
       "|           # Metadata Columns |                                                                                                                                                                                               |         |\n",
       "|                     _spec_id |                                                                                                                                                                                           int |         |\n",
       "|                   _partition |                                                                                                                                                                                      struct<> |         |\n",
       "|                        _file |                                                                                                                                                                                        string |         |\n",
       "|                         _pos |                                                                                                                                                                                        bigint |         |\n",
       "|                     _deleted |                                                                                                                                                                                       boolean |         |\n",
       "|                              |                                                                                                                                                                                               |         |\n",
       "| # Detailed Table Information |                                                                                                                                                                                               |         |\n",
       "|                         Name |                                                                                                                                                                             polaris.nyc.taxis |         |\n",
       "|                         Type |                                                                                                                                                                                       MANAGED |         |\n",
       "|                     Location |                                                                                                                                                                        file:///data/nyc/taxis |         |\n",
       "|                     Provider |                                                                                                                                                                                       iceberg |         |\n",
       "|                        Owner |                                                                                                                                                                                          root |         |\n",
       "|             Table Properties | [created-at=2025-05-01T10:11:24.523358293Z,current-snapshot-id=1227733769855991745,format=iceberg/parquet,format-version=2,write.format.default=parquet,write.parquet.compression-codec=zstd] |         |\n",
       "+------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "DESCRIBE EXTENDED nyc.taxis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcf99fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>cnt</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>2171187</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+---------+\n",
       "|     cnt |\n",
       "+---------+\n",
       "| 2171187 |\n",
       "+---------+"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT COUNT(*) as cnt\n",
    "FROM nyc.taxis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffd2c03",
   "metadata": {},
   "source": [
    "## Schema Evolution\n",
    "\n",
    "Adding, dropping, renaming, or altering columns is easy and safe in Iceberg. In this example, we'll rename `fare_amount` to `fare` and `trip_distance` to `distance`. We'll also add a float column `fare_per_distance_unit` immediately after `distance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efee8252",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "ALTER TABLE nyc.taxis RENAME COLUMN fare_amount TO fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794de3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "ALTER TABLE nyc.taxis RENAME COLUMN trip_distance TO distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adac7564",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "ALTER TABLE nyc.taxis ALTER COLUMN distance COMMENT 'The elapsed trip distance in miles reported by the taximeter.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d7e6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "ALTER TABLE nyc.taxis ALTER COLUMN distance TYPE double;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb4b02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "ALTER TABLE nyc.taxis ALTER COLUMN distance AFTER fare;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f7cc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "ALTER TABLE nyc.taxis\n",
    "ADD COLUMN fare_per_distance_unit float AFTER distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9416b498",
   "metadata": {},
   "source": [
    "Let's update the new `fare_per_distance_unit` to equal `fare` divided by `distance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18771ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "UPDATE nyc.taxis\n",
    "SET fare_per_distance_unit = fare/distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c72ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT\n",
    "VendorID\n",
    ",tpep_pickup_datetime\n",
    ",tpep_dropoff_datetime\n",
    ",fare\n",
    ",distance\n",
    ",fare_per_distance_unit\n",
    "FROM nyc.taxis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37582e02",
   "metadata": {},
   "source": [
    "## Expressive SQL for Row Level Changes\n",
    "With Iceberg tables, `DELETE` queries can be used to perform row-level deletes. This is as simple as providing the table name and a `WHERE` predicate. If the filter matches an entire partition of the table, Iceberg will intelligently perform a metadata-only operation where it simply deletes the metadata for that partition.\n",
    "\n",
    "Let's perform a row-level delete for all rows that have a `fare_per_distance_unit` greater than 4 or a `distance` greater than 2. This should leave us with relatively short trips that have a relatively high fare per distance traveled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded820f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "DELETE FROM nyc.taxis\n",
    "WHERE fare_per_distance_unit > 4.0 OR distance > 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faef3712",
   "metadata": {},
   "source": [
    "There are some fares that have a `null` for `fare_per_distance_unit` due to the distance being `0`. Let's remove those as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b69265",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "DELETE FROM nyc.taxis\n",
    "WHERE fare_per_distance_unit is null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b92d7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT\n",
    "VendorID\n",
    ",tpep_pickup_datetime\n",
    ",tpep_dropoff_datetime\n",
    ",fare\n",
    ",distance\n",
    ",fare_per_distance_unit\n",
    "FROM nyc.taxis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5472b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT COUNT(*) as cnt\n",
    "FROM nyc.taxis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b157e5",
   "metadata": {},
   "source": [
    "## Partitioning\n",
    "\n",
    "A table’s partitioning can be updated in place and applied only to newly written data. Query plans are then split, using the old partition scheme for data written before the partition scheme was changed, and using the new partition scheme for data written after. People querying the table don’t even have to be aware of this split. Simple predicates in WHERE clauses are automatically converted to partition filters that prune out files with no matches. This is what’s referred to in Iceberg as *Hidden Partitioning*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e3e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "ALTER TABLE nyc.taxis\n",
    "ADD PARTITION FIELD VendorID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fce6bb4",
   "metadata": {},
   "source": [
    "## Metadata Tables\n",
    "\n",
    "Iceberg tables contain very rich metadata that can be easily queried. For example, you can retrieve the manifest list for any snapshot, simply by querying the table's `snapshots` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fade1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT snapshot_id, manifest_list\n",
    "FROM nyc.taxis.snapshots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64887133",
   "metadata": {},
   "source": [
    "The `files` table contains loads of information on data files, including column level statistics such as null counts, lower bounds, and upper bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb712f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT file_path, file_format, record_count, null_value_counts, lower_bounds, upper_bounds\n",
    "FROM nyc.taxis.files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65deb074",
   "metadata": {},
   "source": [
    "## Time Travel\n",
    "\n",
    "The history table lists all snapshots and which parent snapshot they derive from. The `is_current_ancestor` flag let's you know if a snapshot is part of the linear history of the current snapshot of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab64f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT *\n",
    "FROM nyc.taxis.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47129d69",
   "metadata": {},
   "source": [
    "You can time-travel by altering the `current-snapshot-id` property of the table to reference any snapshot in the table's history. Let's revert the table to it's original state by traveling to the very first snapshot ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c360238",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql --var df\n",
    "\n",
    "SELECT *\n",
    "FROM nyc.taxis.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df43d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_snapshot = df.head().snapshot_id\n",
    "spark.sql(f\"CALL system.rollback_to_snapshot('nyc.taxis', {original_snapshot})\")\n",
    "original_snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955a4c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT\n",
    "VendorID\n",
    ",tpep_pickup_datetime\n",
    ",tpep_dropoff_datetime\n",
    ",fare\n",
    ",distance\n",
    ",fare_per_distance_unit\n",
    "FROM nyc.taxis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b71c76",
   "metadata": {},
   "source": [
    "Another look at the history table shows that the original state of the table has been added as a new entry\n",
    "with the original snapshot ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b801d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT *\n",
    "FROM nyc.taxis.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85667efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT COUNT(*) as cnt\n",
    "FROM nyc.taxis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
